{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.document_loaders.pdf import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "import pinecone \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all environment variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function for extracting sources from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source(docs):\n",
    "    sources = []\n",
    "    for doc in docs:\n",
    "        sources.append(doc.metadata['source'])\n",
    "    return sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function for formatting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pulling rag_prompt from langchain-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['context', 'question']\n"
     ]
    }
   ],
   "source": [
    "# input variables\n",
    "print(prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# prompt template\n",
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG(query: str , top_k: int=3, source: bool=False) -> str:\n",
    "\n",
    "    # llm model \n",
    "    llm = ChatOpenAI()\n",
    "\n",
    "    # embeddings model\n",
    "    embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "    # prompt \n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # output parser \n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    # loading documents \n",
    "    doc_loader = PyPDFLoader(file_path=\"sample_doc/doc.pdf\")\n",
    "    documents = doc_loader.load()\n",
    "\n",
    "    # chunking (splitting document into chunks)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # embedding documents into vector store \n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_model,\n",
    "    )\n",
    "\n",
    "    # retriever \n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_kwargs={\"k\": top_k},\n",
    "    )\n",
    "\n",
    "    if source:\n",
    "    ### response with source ###\n",
    "        # rag chain from docs\n",
    "        rag_chain_from_docs = (\n",
    "            RunnablePassthrough.assign(context=(lambda x: format_docs(x['context'])))\n",
    "            | prompt\n",
    "            | llm\n",
    "            | output_parser\n",
    "        )\n",
    "        \n",
    "        # retrieved docs\n",
    "        retrieved_docs = (lambda x: x[\"question\"]) | retriever\n",
    "\n",
    "        # rag chain with source\n",
    "        rag_source_chain = RunnablePassthrough.assign(context=retrieved_docs).assign(\n",
    "            answer=rag_chain_from_docs)\n",
    "        \n",
    "        reponse = rag_source_chain.invoke({\"question\": query})\n",
    "\n",
    "        sources = extract_source(reponse['context'])\n",
    "\n",
    "        return reponse['answer'], sources\n",
    "\n",
    "    else:\n",
    "    ### response without source ###\n",
    "        rag_chain = (\n",
    "        {\"context\": retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "        | prompt \n",
    "        | llm\n",
    "        | output_parser\n",
    "        )\n",
    "        # invoke the chain & get the response\n",
    "        reponse = rag_chain.invoke(query)\n",
    "\n",
    "        return reponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG response without Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention is a function that maps a query and key-value pairs to an output using vectors. The output is calculated as a weighted sum.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response without source\n",
    "response = RAG(\"What is Attention?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG response with Source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('An attention mechanism is a function that maps a query and key-value pairs to an output using vectors. It computes the output as a weighted sum of the values based on the query and keys provided.',\n",
       " ['sample_doc/doc.pdf', 'sample_doc/doc.pdf', 'sample_doc/doc.pdf'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAW Response with source\n",
    "RAG(\"What is Attention mechanism?\", source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  An attention mechanism is a function that maps a query and key-value pairs to an output using vectors. The output is calculated as a weighted sum based on the query and key-value pairs.\n",
      "Sources:  ['sample_doc/doc.pdf', 'sample_doc/doc.pdf', 'sample_doc/doc.pdf']\n"
     ]
    }
   ],
   "source": [
    "# Response with sources\n",
    "response, sources = RAG(\"What is Attention mechanism?\", source=True)\n",
    "\n",
    "print(\"Answer: \", response)\n",
    "print(\"Sources: \", sources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_apps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
