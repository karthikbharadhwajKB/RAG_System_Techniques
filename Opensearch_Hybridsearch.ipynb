{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OpenSearchVectorSearch import OpenSearchVectorSearch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_vector_search = OpenSearchVectorSearch(\n",
    "    OPENSEARCH_URL=os.getenv('OPENSEARCH_URL'),\n",
    "    OPENSEARCH_PORT=os.getenv('OPENSEARCH_PORT'),\n",
    "    OPENSEARCH_USERNAME=os.getenv('OPENSEARCH_USERNAME'),\n",
    "    OPENSEARCH_PASSWORD=os.getenv('OPENSEARCH_PASSWORD'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search pipeline search_pipeline_1 created successfully....!\n",
      "Response: {'acknowledged': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "opensearch_vector_search.create_search_pipeline(\n",
    "    search_pipeline_name=\"search_pipeline_1\",\n",
    "    keyword_weight=0.3,\n",
    "    vector_weight=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = opensearch_vector_search.hybrid_search(\n",
    "    query=\"what is attention mechanism?\",\n",
    "    top_k=3, \n",
    "    index_name=\"semantic-index\",\n",
    "    search_pipeline_name=\"search_pipeline_1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'semantic-index',\n",
       "  '_id': 'doc12',\n",
       "  '_score': 0.7,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 12},\n",
       "   'text': 'Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'}},\n",
       " {'_index': 'semantic-index',\n",
       "  '_id': 'doc0',\n",
       "  '_score': 0.56712985,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 0},\n",
       "   'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'}},\n",
       " {'_index': 'semantic-index',\n",
       "  '_id': 'doc3',\n",
       "  '_score': 0.00070000003,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 3},\n",
       "   'text': 'Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment - 1 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Search \n",
    "\n",
    "Keyword_weight: 1, vector_weight: 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search pipeline search_pipeline_keyword_1_vector_0 created successfully....!\n",
      "Response: {'acknowledged': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "opensearch_vector_search.create_search_pipeline(\n",
    "    search_pipeline_name=\"search_pipeline_keyword_1_vector_0\",\n",
    "    keyword_weight=1.0,\n",
    "    vector_weight=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_index': 'semantic-index',\n",
       "  '_id': 'doc0',\n",
       "  '_score': 0.0,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 0},\n",
       "   'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'}},\n",
       " {'_index': 'semantic-index',\n",
       "  '_id': 'doc3',\n",
       "  '_score': 0.0,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 3},\n",
       "   'text': 'Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'}},\n",
       " {'_index': 'semantic-index',\n",
       "  '_id': 'doc4',\n",
       "  '_score': 0.0,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 4},\n",
       "   'text': 'output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5'}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensearch_vector_search.hybrid_search(\n",
    "    query=\"what is Scaled Dot-Product Attention?\",\n",
    "    top_k=3, \n",
    "    index_name=\"semantic-index\",\n",
    "    search_pipeline_name=\"search_pipeline_keyword_1_vector_0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyword Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenSearch client created successfully....!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\opensearchpy\\connection\\http_requests.py:160: UserWarning: Connecting to https://localhost:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n",
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_index': 'semantic-index',\n",
       "  '_id': 'doc3',\n",
       "  '_score': 8.1461115,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 3},\n",
       "   'text': 'Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'}},\n",
       " {'_index': 'semantic-index',\n",
       "  '_id': 'doc0',\n",
       "  '_score': 4.5344005,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 0},\n",
       "   'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'}},\n",
       " {'_index': 'semantic-index',\n",
       "  '_id': 'doc4',\n",
       "  '_score': 4.2044077,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 4},\n",
       "   'text': 'output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5'}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensearch_vector_search.keyword_search(\n",
    "    index_name=\"semantic-index\",\n",
    "    query=\"what is Scaled Dot-Product Attention?\",\n",
    "    top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment - 2\n",
    "\n",
    "#### Hybrid Search \n",
    "\n",
    "keyword_weight = 0.0, vector_weight = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search pipeline search_pipeline_keyword_0_vector_1 created successfully....!\n",
      "Response: {'acknowledged': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create search pipeline\n",
    "opensearch_vector_search.create_search_pipeline(\n",
    "    search_pipeline_name=\"search_pipeline_keyword_0_vector_1\",\n",
    "    keyword_weight=0.0,\n",
    "    vector_weight=1.0,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_index': 'semantic-index',\n",
       "  '_id': 'doc3',\n",
       "  '_score': 1.0,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 3},\n",
       "   'text': 'Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'}},\n",
       " {'_index': 'semantic-index',\n",
       "  '_id': 'doc0',\n",
       "  '_score': 0.004329956,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 0},\n",
       "   'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'}},\n",
       " {'_index': 'semantic-index',\n",
       "  '_id': 'doc4',\n",
       "  '_score': 0.001,\n",
       "  '_source': {'metadata': {'source': 'sample_doc/doc.pdf', 'page': 4},\n",
       "   'text': 'output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensearch_vector_search.hybrid_search(\n",
    "    query=\"what is Scaled Dot-Product Attention?\",\n",
    "    top_k=3, \n",
    "    index_name=\"semantic-index\",\n",
    "    search_pipeline_name=\"search_pipeline_keyword_0_vector_1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\opensearchpy\\connection\\http_requests.py:160: UserWarning: Connecting to https://localhost:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenSearch client created successfully....!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karthik.kolluri\\AppData\\Local\\anaconda3\\envs\\llm_apps\\lib\\site-packages\\urllib3\\connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'_index': 'semantic-index',\n",
       "  '_id': 'doc3',\n",
       "  '_score': 1.8973408,\n",
       "  '_source': {'vector_field': [-0.03521449170297792,\n",
       "    0.019206673187467674,\n",
       "    0.033804826932284815,\n",
       "    -0.03098549925354378,\n",
       "    -0.014720145828003066,\n",
       "    0.030877063501952004,\n",
       "    0.010030301434303873,\n",
       "    -0.022554625155218624,\n",
       "    -0.04036518617829699,\n",
       "    -0.035295818516671754,\n",
       "    0.0601275944553177,\n",
       "    0.025780586902428824,\n",
       "    0.0008708741643101675,\n",
       "    0.019450653628549173,\n",
       "    -0.011643282307908972,\n",
       "    0.0028854061805285674,\n",
       "    0.008627414829407843,\n",
       "    0.017403928817254378,\n",
       "    -0.006540028008250011,\n",
       "    -0.02500798217233741,\n",
       "    -0.022676615375759372,\n",
       "    0.01573673099917598,\n",
       "    -0.0008920530220429365,\n",
       "    -0.006746733194060542,\n",
       "    -0.016265353425540717,\n",
       "    0.0183662961126314,\n",
       "    0.032747578354264985,\n",
       "    -0.0348349665724067,\n",
       "    -0.00696699331448134,\n",
       "    -0.013601902139712862,\n",
       "    0.027258022155221613,\n",
       "    0.007387181851899477,\n",
       "    -0.004093447294283123,\n",
       "    0.011162099591543049,\n",
       "    -0.010789351695446314,\n",
       "    0.023598317401644305,\n",
       "    0.0069941022523792844,\n",
       "    -0.008369879919377371,\n",
       "    0.009969306324033499,\n",
       "    -0.01076224275754837,\n",
       "    0.015438531750976002,\n",
       "    0.006174057346627762,\n",
       "    0.013235932409413202,\n",
       "    -0.0072041965210883525,\n",
       "    0.0022974813226975467,\n",
       "    0.012124465955597484,\n",
       "    0.0006671336693362532,\n",
       "    -0.039985661047725767,\n",
       "    0.0026736176032008774,\n",
       "    0.018569613146865983,\n",
       "    0.017132839438274934,\n",
       "    0.049609330276205656,\n",
       "    -0.03643439390838542,\n",
       "    -0.012571763430913565,\n",
       "    -0.017187057314070824,\n",
       "    0.0001764197976024732,\n",
       "    0.01667198749400988,\n",
       "    0.0014935322324076056,\n",
       "    0.022202208962545347,\n",
       "    -0.02331367355371589,\n",
       "    -0.0029480953665869166,\n",
       "    0.006573914180622441,\n",
       "    -0.010674138709380051,\n",
       "    0.01488279945539073,\n",
       "    -0.009806652696645832,\n",
       "    -0.012592095134337023,\n",
       "    0.004994819013728478,\n",
       "    0.0006794174068212592,\n",
       "    0.014842136048543815,\n",
       "    -0.011345084922354172,\n",
       "    0.0407989291846641,\n",
       "    0.028599914581169856,\n",
       "    0.0011157016432856537,\n",
       "    -0.011033332136527813,\n",
       "    0.02339500036740972,\n",
       "    -0.015804503343920837,\n",
       "    -3.388615418253611e-05,\n",
       "    -0.01450327432481951,\n",
       "    0.004239157835484574,\n",
       "    -0.002424554469094161,\n",
       "    0.011026554902053327,\n",
       "    -0.0010868983967690878,\n",
       "    0.007841256561690045,\n",
       "    0.030524647309278726,\n",
       "    0.0046288488177675235,\n",
       "    -0.0009691440059825539,\n",
       "    0.005933465522783506,\n",
       "    0.024926655358643576,\n",
       "    -0.0072719688658332135,\n",
       "    0.0065298621565382815,\n",
       "    0.008268221402260079,\n",
       "    -0.02464201151071516,\n",
       "    0.02395073359431758,\n",
       "    0.0011512821242767057,\n",
       "    -0.01654999727346913,\n",
       "    0.01259887236881151,\n",
       "    0.01369000618788118,\n",
       "    0.0407989291846641,\n",
       "    -0.01424573941478904,\n",
       "    0.01438128410427876,\n",
       "    -0.03892841619499629,\n",
       "    -0.009488123607667574,\n",
       "    -0.028274607326394523,\n",
       "    -0.01648222492872427,\n",
       "    -0.012903847920163384,\n",
       "    -0.007522726541389199,\n",
       "    -0.009901533979288637,\n",
       "    0.02644475588092846,\n",
       "    -0.023937179125368607,\n",
       "    -0.005855527326326916,\n",
       "    -0.012659867479081885,\n",
       "    0.03383193587018276,\n",
       "    0.019857285834373163,\n",
       "    -0.04134110794262299,\n",
       "    -0.014530383262717455,\n",
       "    -0.003236127831752575,\n",
       "    -0.001419829923912893,\n",
       "    -0.012110911486648512,\n",
       "    -0.02083320759869916,\n",
       "    -0.010192955061691539,\n",
       "    0.03792538549277235,\n",
       "    0.007305855038205644,\n",
       "    0.025712814557683962,\n",
       "    -0.022242872369392264,\n",
       "    0.01772923607202971,\n",
       "    0.018962692746386175,\n",
       "    0.022635951968912455,\n",
       "    -0.018434068457376263,\n",
       "    -0.00616050287767879,\n",
       "    -0.02439803106963366,\n",
       "    0.019396435752753287,\n",
       "    0.03594643302622242,\n",
       "    -0.025184190268674047,\n",
       "    -0.013141051126770397,\n",
       "    -0.02109074250872963,\n",
       "    -0.008071681602499983,\n",
       "    -0.002370336593298272,\n",
       "    -0.011799158700822153,\n",
       "    -0.043211624657581146,\n",
       "    -0.02414049615960319,\n",
       "    0.012266787879561692,\n",
       "    0.013066501547551049,\n",
       "    -0.010003192496405928,\n",
       "    -0.01036238592355369,\n",
       "    -0.002663451751489148,\n",
       "    0.046600238169533834,\n",
       "    0.010233618468538454,\n",
       "    0.06218787001027112,\n",
       "    -0.006380762998099588,\n",
       "    -0.00639431746704856,\n",
       "    0.014421947511125677,\n",
       "    -0.00982698440006929,\n",
       "    -0.01297839749938273,\n",
       "    0.01358834767076389,\n",
       "    0.018515395271070094,\n",
       "    0.005221856368623762,\n",
       "    -0.02114496038452552,\n",
       "    0.016943076872989325,\n",
       "    -0.023178128864226166,\n",
       "    -0.009515232545565518,\n",
       "    0.0038596329377440005,\n",
       "    0.019084682966926926,\n",
       "    -0.013405362339952766,\n",
       "    0.0004943143066521816,\n",
       "    0.0143135117595339,\n",
       "    0.021619366797739546,\n",
       "    0.01554696750256778,\n",
       "    -0.009108598477096354,\n",
       "    -0.002575347703320829,\n",
       "    -0.010436935502773038,\n",
       "    -0.010457267206196497,\n",
       "    0.01905757402902898,\n",
       "    -0.0419646135142757,\n",
       "    0.0035885437915952043,\n",
       "    -0.022364862589933015,\n",
       "    -0.021998891928310765,\n",
       "    0.04548877544100847,\n",
       "    -0.02339500036740972,\n",
       "    -0.02500798217233741,\n",
       "    -0.026214329908795932,\n",
       "    0.010057410372201817,\n",
       "    -0.0021585480159705818,\n",
       "    0.01779700841677457,\n",
       "    0.02712247746573189,\n",
       "    0.010497930613043412,\n",
       "    -0.028789677146455465,\n",
       "    0.018962692746386175,\n",
       "    -0.0029125148855958646,\n",
       "    0.0011055357915739246,\n",
       "    -0.035512690019855304,\n",
       "    0.01890847487059029,\n",
       "    0.023964288063266552,\n",
       "    -0.04212726714166337,\n",
       "    -0.035810888336732696,\n",
       "    -0.5950951067124457,\n",
       "    -0.007454954196644338,\n",
       "    -0.009718549579800102,\n",
       "    -0.05028705372365426,\n",
       "    -0.0064993646014030945,\n",
       "    -0.012592095134337023,\n",
       "    0.02381518890482786,\n",
       "    0.011345084922354172,\n",
       "    -0.019003356153233092,\n",
       "    0.023733862091134025,\n",
       "    -0.007366850148476019,\n",
       "    -0.0038494670860322712,\n",
       "    -0.0070279884247517145,\n",
       "    -0.008552865250188495,\n",
       "    -0.009244143166586075,\n",
       "    -0.01594004803341056,\n",
       "    -0.0019586198318038896,\n",
       "    -0.026919160431497308,\n",
       "    0.011575510894486699,\n",
       "    0.025848359247173682,\n",
       "    -0.050259944785756315,\n",
       "    0.003951125835980209,\n",
       "    0.009874425041390692,\n",
       "    0.012747971527250204,\n",
       "    -0.011507738549741839,\n",
       "    -0.017322602003560544,\n",
       "    0.017336156472509516,\n",
       "    -9.122787875219846e-05,\n",
       "    0.0076989341720645425,\n",
       "    0.02109074250872963,\n",
       "    -0.024303149786990857,\n",
       "    0.002939623823493809,\n",
       "    0.01942354469065123,\n",
       "    0.012226124472714775,\n",
       "    0.034807857634508756,\n",
       "    -0.009542341483463463,\n",
       "    -0.019328663408008425,\n",
       "    0.015858721219716727,\n",
       "    -0.009555895952412435,\n",
       "    0.05145273805326587,\n",
       "    -0.015899384626563644,\n",
       "    -0.024980873234439465,\n",
       "    0.03453676825552931,\n",
       "    -0.0074142907897974215,\n",
       "    -0.005672542461177086,\n",
       "    0.010342054220130233,\n",
       "    0.010714802116226966,\n",
       "    -0.0074481769621698515,\n",
       "    -0.008817177394693452,\n",
       "    -0.004388256993923268,\n",
       "    -0.016644878556111936,\n",
       "    -0.011995698500582248,\n",
       "    -0.006404483318760289,\n",
       "    -0.009020494428928034,\n",
       "    0.0014316900842432437,\n",
       "    0.0028244110702581927,\n",
       "    0.03188009606682112,\n",
       "    0.0008590140039798169,\n",
       "    0.019342217876957397,\n",
       "    0.006895832352499236,\n",
       "    0.006085953298459444,\n",
       "    0.013385030636529308,\n",
       "    -0.01493701733118662,\n",
       "    -0.012117688721122998,\n",
       "    -0.012151574893495429,\n",
       "    -0.0022517349899947654,\n",
       "    -0.009196702525264673,\n",
       "    0.025631487743990128,\n",
       "    0.0010419992183756176,\n",
       "    -0.01007096484115079,\n",
       "    0.0026973379238615786,\n",
       "    0.015574076440465724,\n",
       "    0.011833044873194582,\n",
       "    0.009698217876376643,\n",
       "    0.00023593235872772323,\n",
       "    0.007400736320848449,\n",
       "    0.024994427703388437,\n",
       "    0.01438128410427876,\n",
       "    -0.001948453863676837,\n",
       "    0.01448971985587054,\n",
       "    0.008261444167785594,\n",
       "    -0.026133003095102098,\n",
       "    -0.012646313010132913,\n",
       "    0.004391645611160511,\n",
       "    0.0020704439678022627,\n",
       "    0.024303149786990857,\n",
       "    -0.009284806573432992,\n",
       "    0.03787116761697647,\n",
       "    0.003510605595138614,\n",
       "    -0.02464201151071516,\n",
       "    0.0022551236072320085,\n",
       "    0.00719064205213938,\n",
       "    -0.017268384127764654,\n",
       "    -0.05779622579609449,\n",
       "    0.01535720493728217,\n",
       "    0.04443152686298864,\n",
       "    -0.005953797226206965,\n",
       "    0.004662734990139954,\n",
       "    0.022608843031014514,\n",
       "    -0.02389651571852169,\n",
       "    -0.03182587819102523,\n",
       "    -0.026919160431497308,\n",
       "    -0.0029870644648152113,\n",
       "    -0.007793815454707348,\n",
       "    0.015099670958574287,\n",
       "    0.015641849716533172,\n",
       "    -0.013988204504758569,\n",
       "    0.0027583330341319533,\n",
       "    0.016644878556111936,\n",
       "    -0.009569450421361408,\n",
       "    -0.019369326814855342,\n",
       "    -0.017756345009927652,\n",
       "    -0.004062949739147936,\n",
       "    0.0008285165070522913,\n",
       "    0.02202600086620871,\n",
       "    -0.028166171574802747,\n",
       "    0.046193607826355024,\n",
       "    -0.010592811895686217,\n",
       "    -0.008695187174152703,\n",
       "    -0.0005684402505092114,\n",
       "    0.017959662044162238,\n",
       "    -0.009488123607667574,\n",
       "    0.0032479879920829257,\n",
       "    -0.02363898080849122,\n",
       "    0.014665927952207176,\n",
       "    0.00443569763524467,\n",
       "    -0.00868163270520373,\n",
       "    -0.004655957755665468,\n",
       "    -0.005106643382557499,\n",
       "    0.011846599342143554,\n",
       "    -0.012314228520883094,\n",
       "    -0.0075972756549472515,\n",
       "    0.011494184080792866,\n",
       "    -0.009901533979288637,\n",
       "    -0.0005167638376412549,\n",
       "    0.01641445258397941,\n",
       "    -0.013717115125779125,\n",
       "    -0.010802906164395287,\n",
       "    0.011256980874185854,\n",
       "    -0.016387343646081465,\n",
       "    0.0018400182285003832,\n",
       "    -0.005597992881957739,\n",
       "    0.008295330340158023,\n",
       "    -0.027339348968915444,\n",
       "    -0.018596722084763928,\n",
       "    -0.01029461357880883,\n",
       "    -0.024669120448613104,\n",
       "    0.0037918605929991396,\n",
       "    0.017485255630948212,\n",
       "    0.00895949931865766,\n",
       "    0.021429604232453933,\n",
       "    -0.005628490437092927,\n",
       "    -0.004584796793683364,\n",
       "    -0.015831612281818782,\n",
       "    -0.01672620536980577,\n",
       "    -0.006523084922063795,\n",
       "    -0.0030616140440345584,\n",
       "    -0.026661627384112013,\n",
       "    -0.008356325450428399,\n",
       "    -0.04516346818623314,\n",
       "    0.010145514420370136,\n",
       "    0.024628457041766187,\n",
       "    -0.04424176429770303,\n",
       "    -0.009345801683703367,\n",
       "    -0.01824430589209065,\n",
       "    -0.005591215647483254,\n",
       "    -0.016387343646081465,\n",
       "    0.010809683398869773,\n",
       "    -0.03109393500513556,\n",
       "    -0.022324199183086098,\n",
       "    -0.004916880817271888,\n",
       "    -0.04166641519739832,\n",
       "    -0.0020755268936581274,\n",
       "    0.01848828633317215,\n",
       "    -0.010592811895686217,\n",
       "    0.0016087452184738692,\n",
       "    0.007658270765217626,\n",
       "    0.007299077803731158,\n",
       "    0.007305855038205644,\n",
       "    -0.013364699864428438,\n",
       "    0.028464369891680136,\n",
       "    -0.010443712737247524,\n",
       "    -0.030958390315645838,\n",
       "    -0.0031666611783890924,\n",
       "    0.048768953201369376,\n",
       "    0.013839105346319875,\n",
       "    0.02138894082560702,\n",
       "    0.006265550012033325,\n",
       "    -0.02987403466237324,\n",
       "    0.005445505106281803,\n",
       "    0.016089145329204076,\n",
       "    0.011399302798150062,\n",
       "    -0.027081814058884972,\n",
       "    0.010436935502773038,\n",
       "    -0.013561238732865945,\n",
       "    -0.007346518445052561,\n",
       "    0.0322053995963061,\n",
       "    0.0018739044008728135,\n",
       "    -0.01341213957442725,\n",
       "    0.022703724313657317,\n",
       "    0.027583329409996943,\n",
       "    0.0038393012343205424,\n",
       "    0.009881202275865178,\n",
       "    -0.02316457625792237,\n",
       "    0.019030465091131037,\n",
       "    -0.022297090245188153,\n",
       "    0.001644325699464921,\n",
       "    -0.016943076872989325,\n",
       "    0.009433905731871686,\n",
       "    -0.0003979505330719135,\n",
       "    -0.0068551689456523195,\n",
       "    -0.01636023470818352,\n",
       "    -0.0229070413478919,\n",
       "    -0.015113225427523257,\n",
       "    -0.009020494428928034,\n",
       "    0.05047681628893987,\n",
       "    -0.019450653628549173,\n",
       "    0.0046661236073771975,\n",
       "    0.012490436617219733,\n",
       "    0.025550160930296297,\n",
       "    0.009264474870009533,\n",
       "    -0.023788079966929915,\n",
       "    0.00965077723505524,\n",
       "    0.003734254332796655,\n",
       "    0.009122152946045325,\n",
       "    0.0004714411403007911,\n",
       "    -0.0063570426774388865,\n",
       "    0.019369326814855342,\n",
       "    0.012293896817459637,\n",
       "    -0.013771333001575013,\n",
       "    -0.009034048897877007,\n",
       "    -0.006675572697739733,\n",
       "    0.032856014105856764,\n",
       "    0.023842297842725804,\n",
       "    -0.0023059528657906544,\n",
       "    -0.004388256993923268,\n",
       "    0.02533328942711274,\n",
       "    0.008464761202020176,\n",
       "    0.037518751424303186,\n",
       "    -0.0066010231185203855,\n",
       "    0.0027854419720298976,\n",
       "    0.03399458949757043,\n",
       "    -0.011284089812083798,\n",
       "    -0.015031898613829425,\n",
       "    0.004198494428637657,\n",
       "    -0.01573673099917598,\n",
       "    0.028952330773843134,\n",
       "    0.011080772777849215,\n",
       "    -0.013568015967340431,\n",
       "    -0.012985174733857217,\n",
       "    -0.007922583375383877,\n",
       "    -0.017634354789386904,\n",
       "    0.004767782124494488,\n",
       "    -0.011114658950221646,\n",
       "    0.014855690517492788,\n",
       "    0.012138020424546456,\n",
       "    -0.002494020889626996,\n",
       "    0.0003198005771418267,\n",
       "    0.019084682966926926,\n",
       "    0.024181159566450106,\n",
       "    0.004510247214464017,\n",
       "    0.0019620084490411327,\n",
       "    -0.005706428633549517,\n",
       "    -0.012524322789592163,\n",
       "    0.01654999727346913,\n",
       "    -0.003818969530897084,\n",
       "    0.0019501481722954586,\n",
       "    -0.006743344576823299,\n",
       "    -0.02687849702465039,\n",
       "    -0.008566419719137467,\n",
       "    -0.010552148488839301,\n",
       "    -0.019613305393291664,\n",
       "    0.0023872796794844873,\n",
       "    -0.031554788812045785,\n",
       "    -0.01505900755172737,\n",
       "    0.028328825202190412,\n",
       "    -0.01488279945539073,\n",
       "    0.008505424608867093,\n",
       "    0.004113778997706582,\n",
       "    0.029358964842312298,\n",
       "    -0.008715518877576161,\n",
       "    -0.031663224563637564,\n",
       "    0.003239516448989818,\n",
       "    0.009420351262922714,\n",
       "    0.0026329541963539607,\n",
       "    -0.014760809234849981,\n",
       "    -0.023300119084766917,\n",
       "    -0.007332963976103588,\n",
       "    -0.0029972303165269405,\n",
       "    0.00035982861825676016,\n",
       "    -0.022934150285789844,\n",
       "    0.01304616984412759,\n",
       "    0.0006925482986155759,\n",
       "    -0.015574076440465724,\n",
       "    -0.02400495147011347,\n",
       "    0.000587077645314048,\n",
       "    0.01627890789448969,\n",
       "    -0.00045873382566112965,\n",
       "    0.01892202933953926,\n",
       "    -0.018637385491610845,\n",
       "    -0.0067840079836702155,\n",
       "    -0.0018383239198817617,\n",
       "    -0.020697662909209436,\n",
       "    -0.027203804279425724,\n",
       "    0.0467628917969215,\n",
       "    0.0013308787214352633,\n",
       "    -0.01352057532601903,\n",
       "    -0.01636023470818352,\n",
       "    0.009115375711570839,\n",
       "    -0.00436792529049981,\n",
       "    0.01800032545100915,\n",
       "    -0.0069263299076344235,\n",
       "    -0.023855852311674777,\n",
       "    0.024425140007531605,\n",
       "    0.026431201411979487,\n",
       "    -0.0009123847254663948,\n",
       "    0.010403049330400607,\n",
       "    0.012727639823826745,\n",
       "    0.016509333866622216,\n",
       "    0.007055097362649659,\n",
       "    0.0032242676714222245,\n",
       "    -0.028843895022251355,\n",
       "    0.004256100921670789,\n",
       "    0.02202600086620871,\n",
       "    0.03743742461060935,\n",
       "    0.03323553923642798,\n",
       "    -0.024221822973297023,\n",
       "    0.02609233968825518,\n",
       "    -0.03060597412297256,\n",
       "    0.0067162356389253546,\n",
       "    -0.020250365433893355,\n",
       "    -0.027339348968915444,\n",
       "    0.023300119084766917,\n",
       "    -0.001647714316702164,\n",
       "    -0.02029102884074027,\n",
       "    -0.04112423643943943,\n",
       "    -0.0067535104285350285,\n",
       "    -0.015912939095512616,\n",
       "    0.04193750457637776,\n",
       "    0.01436772963532979,\n",
       "    -0.0013486689619307893,\n",
       "    0.023584762932695333,\n",
       "    0.0016756702924940958,\n",
       "    0.03206985863210673,\n",
       "    0.01549274962677189,\n",
       "    -0.0007611677394620857,\n",
       "    0.019477762566447118,\n",
       "    0.010043855903252845,\n",
       "    0.003180215647338065,\n",
       "    -0.00887139527048934,\n",
       "    0.010369163158028176,\n",
       "    0.0031700497956263355,\n",
       "    0.011650059542383458,\n",
       "    -0.027651101754741805,\n",
       "    -0.004679678076326169,\n",
       "    0.01535720493728217,\n",
       "    -0.00036237005208086153,\n",
       "    0.010382717626977149,\n",
       "    -0.01069447041280351,\n",
       "    0.0434284961607647,\n",
       "    1.1006382171030104e-05,\n",
       "    0.007990355720128739,\n",
       "    0.01771568160308074,\n",
       "    -0.0013452803446935462,\n",
       "    0.009122152946045325,\n",
       "    0.018461177395274208,\n",
       "    -0.006540028008250011,\n",
       "    -0.004476361042091587,\n",
       "    -0.005818252536717243,\n",
       "    -0.005330292120215539,\n",
       "    -0.016211135549744828,\n",
       "    0.009684663407427671,\n",
       "    -0.020629890564464577,\n",
       "    0.009698217876376643,\n",
       "    0.011656836776857945,\n",
       "    -0.00724485992793527,\n",
       "    -0.04372669447764209,\n",
       "    0.0036020982605441763,\n",
       "    0.01304616984412759,\n",
       "    -0.0127005308859288,\n",
       "    -0.004774559358968975,\n",
       "    -0.013283373050734603,\n",
       "    -0.008546088015714009,\n",
       "    -0.0360006509020183,\n",
       "    -0.015194551309894503,\n",
       "    -0.011087550012323701,\n",
       "    0.021321168480862157,\n",
       "    0.0005061744087748705,\n",
       "    0.020928088881341966,\n",
       "    -0.008783291222321021,\n",
       "    -0.03301866773324443,\n",
       "    -0.010836792336767716,\n",
       "    -0.019152455311671788,\n",
       "    -0.01070802488175248,\n",
       "    -0.009935420151661068,\n",
       "    -0.014083085787401374,\n",
       "    -0.038304910623343576,\n",
       "    -0.025373952833959657,\n",
       "    0.03055175624717667,\n",
       "    0.020711217378158408,\n",
       "    0.02362542633954225,\n",
       "    0.010009969730880414,\n",
       "    -0.026119448626153126,\n",
       "    -0.004794891062392433,\n",
       "    -0.013961095566860624,\n",
       "    -0.02550949752344938,\n",
       "    -0.01686175005929549,\n",
       "    -0.02222931790044329,\n",
       "    -0.016265353425540717,\n",
       "    0.026553191632520234,\n",
       "    0.00845798396754569,\n",
       "    -0.015411422813078057,\n",
       "    -0.016631324087162964,\n",
       "    -0.008979831022081119,\n",
       "    -0.00908148953919841,\n",
       "    0.001040304909756996,\n",
       "    0.0009699911602918647,\n",
       "    -0.015858721219716727,\n",
       "    0.016319571301336603,\n",
       "    0.02469622938651105,\n",
       "    0.018759375712151593,\n",
       "    0.017566582444642043,\n",
       "    0.0381964748717518,\n",
       "    -0.025997458405612378,\n",
       "    -0.001043693526994239,\n",
       "    -0.012056693610852624,\n",
       "    0.015452086219924974,\n",
       "    -0.01810876120260093,\n",
       "    0.019179564249569733,\n",
       "    -0.0183662961126314,\n",
       "    0.006878889266313021,\n",
       "    0.011507738549741839,\n",
       "    0.010308168047757802,\n",
       "    -0.022378417058881987,\n",
       "    0.008390211622800828,\n",
       "    -0.019857285834373163,\n",
       "    -0.0035919324088324474,\n",
       "    -0.018135870140498875,\n",
       "    0.03535003639246764,\n",
       "    -0.00375119741898287,\n",
       "    0.02072477184710738,\n",
       "    0.0047711707417317315,\n",
       "    -0.01935577234590637,\n",
       "    0.0010631780761083866,\n",
       "    0.0026498972825401757,\n",
       "    -0.02258173409311657,\n",
       "    0.04708819905169683,\n",
       "    0.012781857699622635,\n",
       "    -0.010342054220130233,\n",
       "    0.015519858564669834,\n",
       "    0.014042422380554457,\n",
       "    -0.03084995456405406,\n",
       "    -0.03152767987414785,\n",
       "    0.014869244986441758,\n",
       "    0.004262878156145275,\n",
       "    0.007610830123896224,\n",
       "    -0.0129445113270103,\n",
       "    -0.002433026012187268,\n",
       "    -0.017620800320437932,\n",
       "    0.0050930889136085265,\n",
       "    -0.02336789142951178,\n",
       "    0.016956631341938297,\n",
       "    -0.022351308120984043,\n",
       "    0.0054116189339093726,\n",
       "    0.002600762332600152,\n",
       "    0.01561473984731264,\n",
       "    0.021754911487229266,\n",
       "    -0.005452282340756289,\n",
       "    0.017065067093530072,\n",
       "    -0.022202208962545347,\n",
       "    -0.02545527964765349,\n",
       "    -0.0006116451202840601,\n",
       "    -0.0016544915511766502,\n",
       "    0.02381518890482786,\n",
       "    -0.01213124319007197,\n",
       "    -0.011304421515507257,\n",
       "    -0.024791110669153855,\n",
       "    -0.017024403686683155,\n",
       "    -0.0066010231185203855,\n",
       "    -0.04250679227223459,\n",
       "    1.2488901302829218e-05,\n",
       "    0.0005472613927764423,\n",
       "    0.03732898885901758,\n",
       "    0.02141604976350496,\n",
       "    0.05101899504689876,\n",
       "    0.014584601138513344,\n",
       "    0.027502002596303112,\n",
       "    0.025875468185071627,\n",
       "    -0.008308884809106996,\n",
       "    -0.00424593506995906,\n",
       "    0.005984294781342152,\n",
       "    -0.0076989341720645425,\n",
       "    -0.011927926155837388,\n",
       "    -0.0038765760239302155,\n",
       "    0.009400019559499255,\n",
       "    0.004273044007857004,\n",
       "    0.007644716296268654,\n",
       "    0.009020494428928034,\n",
       "    0.014516828793768482,\n",
       "    0.03291023198165265,\n",
       "    -0.002038252104048454,\n",
       "    -0.026959823838344225,\n",
       "    -0.009684663407427671,\n",
       "    -0.02900654864963902,\n",
       "    0.0067128470216881115,\n",
       "    0.013066501547551049,\n",
       "    -0.018867811463743372,\n",
       "    -0.00861386036045887,\n",
       "    -0.0324493800373876,\n",
       "    0.010491153378568926,\n",
       "    0.020656999502362522,\n",
       "    0.0168075321834996,\n",
       "    0.012503991086168705,\n",
       "    0.021212732729270378,\n",
       "    0.046546020293737944,\n",
       "    -0.018081652264702985,\n",
       "    -0.012395555334576928,\n",
       "    0.015167442371996558,\n",
       "    0.021687139142484404,\n",
       "    -0.016536442804520157,\n",
       "    -0.0048016682968669185,\n",
       "    -0.028003517947415082,\n",
       "    -0.011873708280041499,\n",
       "    0.03570245258514092,\n",
       "    0.005469225426942504,\n",
       "    0.01523521471674142,\n",
       "    -0.0014223713868408254,\n",
       "    0.012022807438480193,\n",
       "    -0.015777394406022893,\n",
       "    0.005049036889524367,\n",
       "    -0.008031018195653066,\n",
       "    -0.030768627750360225,\n",
       "    -0.007285523334782186,\n",
       "    -0.038684435753914795,\n",
       "    -0.027637547285792832,\n",
       "    -0.010558925723313788,\n",
       "    -0.05055814310263371,\n",
       "    -0.02881678608435341,\n",
       "    -0.000589195531087325,\n",
       "    -0.0003325078917814881,\n",
       "    -0.021307614011913185,\n",
       "    0.0005828418737674943,\n",
       "    -0.013310481988632548,\n",
       "    -0.02077898972290327,\n",
       "    0.02178202042512721,\n",
       "    0.0030616140440345584,\n",
       "    0.05345879945771375,\n",
       "    -0.029277638028618463,\n",
       "    0.001240233210339012,\n",
       "    0.02296125922368779,\n",
       "    0.017769899478876625,\n",
       "    -0.018949138277437206,\n",
       "    0.013906877691064735,\n",
       "    -0.016902413466142407,\n",
       "    0.010213286765114998,\n",
       "    0.01148740684631838,\n",
       "    0.02559082433714321,\n",
       "    -0.04012120573721549,\n",
       "    -0.02259528856206554,\n",
       "    -0.0007099149619564465,\n",
       "    -0.022147991086749457,\n",
       "    -0.009677886172953185,\n",
       "    -0.0467628917969215,\n",
       "    0.006526473539301038,\n",
       "    0.021809129363025155,\n",
       "    0.017065067093530072,\n",
       "    -0.0016087452184738692,\n",
       "    -0.025102863454980213,\n",
       "    0.004540744769599205,\n",
       "    0.016509333866622216,\n",
       "    -0.004564465090259906,\n",
       "    0.008105567774872414,\n",
       "    -0.00145371597987,\n",
       "    -0.012449773210372815,\n",
       "    -0.024181159566450106,\n",
       "    0.028220389450598637,\n",
       "    -0.0034292787814447813,\n",
       "    0.028545696705373966,\n",
       "    0.019084682966926926,\n",
       "    0.00459157402815785,\n",
       "    -0.004276432625094247,\n",
       "    -0.00970499511085113,\n",
       "    -0.01424573941478904,\n",
       "    0.016631324087162964,\n",
       "    0.009664331704004213,\n",
       "    0.015397868344129085,\n",
       "    -0.036244631343099805,\n",
       "    0.034075916311264255,\n",
       "    0.025021536641286382,\n",
       "    -0.018515395271070094,\n",
       "    0.023747416560082998,\n",
       "    0.0068551689456523195,\n",
       "    0.004649180521190982,\n",
       "    0.030090906165556795,\n",
       "    -0.030524647309278726,\n",
       "    -0.002031474869573968,\n",
       "    0.017566582444642043,\n",
       "    -0.023300119084766917,\n",
       "    0.024858883013898714,\n",
       "    -0.0011140073346670321,\n",
       "    -0.0037240884810849258,\n",
       "    -0.013249486878362174,\n",
       "    -0.025726369026632934,\n",
       "    -0.01606203639130613,\n",
       "    0.02981981678657735,\n",
       "    0.014598155607462316,\n",
       "    -0.018949138277437206,\n",
       "    0.007956469547756308,\n",
       "    -0.0013859437515404626,\n",
       "    0.0005070215630841812,\n",
       "    -0.01989794924122008,\n",
       "    -0.0074481769621698515,\n",
       "    0.02569926008873499,\n",
       "    -0.04060916661937849,\n",
       "    -0.02843726095378219,\n",
       "    0.010809683398869773,\n",
       "    -0.019735295613832412,\n",
       "    0.002592290789507044,\n",
       "    0.0025414615309483986,\n",
       "    -0.010131959951421163,\n",
       "    0.007780260985758376,\n",
       "    0.012707308120403287,\n",
       "    -0.012497213851694219,\n",
       "    -0.010321722516706775,\n",
       "    0.025360398365010684,\n",
       "    -0.015316541530435253,\n",
       "    -0.018447622926325236,\n",
       "    0.004506858597226774,\n",
       "    -0.021998891928310765,\n",
       "    -0.019586198318038896,\n",
       "    0.010579257426737246,\n",
       "    -0.01983017689647522,\n",
       "    -0.03743742461060935,\n",
       "    -0.013337590926530493,\n",
       "    -0.010131959951421163,\n",
       "    0.0197895134896283,\n",
       "    0.022812160065249096,\n",
       "    -0.003808803679185355,\n",
       "    -0.013554461498391459,\n",
       "    0.0034208072383516736,\n",
       "    0.014964126269084565,\n",
       "    -0.01219901553481683,\n",
       "    0.01715994837617288,\n",
       "    0.0034004755349282155,\n",
       "    0.001797660513034845,\n",
       "    -0.020209702027046438,\n",
       "    0.025997458405612378,\n",
       "    -0.008607083125984384,\n",
       "    0.036190413467303915,\n",
       "    -0.025129972392878158,\n",
       "    -0.0072041965210883525,\n",
       "    -0.007075429066073117,\n",
       "    -0.040256750426705214,\n",
       "    0.016699096431907826,\n",
       "    -0.0196946322069855,\n",
       "    0.006065621595035985,\n",
       "    -0.022934150285789844,\n",
       "    0.005811475302242757,\n",
       "    0.01866449442950879,\n",
       "    -0.0007628620480807073,\n",
       "    -0.022012446397259737,\n",
       "    0.023178128864226166,\n",
       "    0.010023524199829386,\n",
       "    -0.0005616630160347253,\n",
       "    0.01972174114488344,\n",
       "    0.005204913282437547,\n",
       "    -0.011812713169771125,\n",
       "    0.005343846589164512,\n",
       "    -0.03250359791318349,\n",
       "    -0.03944348228976688,\n",
       "    0.005262519775470678,\n",
       "    0.00028676158818253795,\n",
       "    -0.004639014669479253,\n",
       "    -0.017187057314070824,\n",
       "    -0.015506304095720862,\n",
       "    0.028328825202190412,\n",
       "    0.03282890516795882,\n",
       "    -0.004639014669479253,\n",
       "    -0.00037931313826707676,\n",
       "    -0.018962692746386175,\n",
       "    -0.03903685194658807,\n",
       "    -0.02336789142951178,\n",
       "    -0.03098549925354378,\n",
       "    -0.006431592256658234,\n",
       "    0.020114820744403634,\n",
       "    0.01996572158596494,\n",
       "    0.005672542461177086,\n",
       "    0.01138574832920109,\n",
       "    -0.007658270765217626,\n",
       "    -0.008979831022081119,\n",
       "    -0.02316457625792237,\n",
       "    0.024926655358643576,\n",
       "    0.013974650035809597,\n",
       "    -0.006465478429030664,\n",
       "    0.0041103903804693386,\n",
       "    -0.01804098885785607,\n",
       "    -0.019396435752753287,\n",
       "    -0.01219901553481683,\n",
       "    -0.015072562020676342,\n",
       "    -0.003024339254424885,\n",
       "    -0.0034648592624358333,\n",
       "    0.02311035838212648,\n",
       "    0.012890293451214412,\n",
       "    -0.0037850835913553005,\n",
       "    -0.008878172504963826,\n",
       "    0.0512900844258782,\n",
       "    0.0006175751422415736,\n",
       "    0.011521293018690811,\n",
       "    0.009840538869018263,\n",
       "    -0.03437411462814165,\n",
       "    0.008966276553132146,\n",
       "    -0.002656674517014662,\n",
       "    -0.0006976312244714406,\n",
       "    -0.012144797659020943,\n",
       "    0.018325632705784484,\n",
       "    0.03553979895775325,\n",
       "    -0.014747254765901009,\n",
       "    0.004781336593443461,\n",
       "    -0.028355934140088357,\n",
       "    -0.001797660513034845,\n",
       "    0.0014740476832934583,\n",
       "    -0.0048389430864765924,\n",
       "    0.03787116761697647,\n",
       "    0.009339024449228881,\n",
       "    0.020914534412392993,\n",
       "    0.037410315672711414,\n",
       "    0.019857285834373163,\n",
       "    0.0015757062004107495,\n",
       "    -0.0026261769618794745,\n",
       "    0.0018129092906024387,\n",
       "    -0.014069531318452401,\n",
       "    -0.013947541097911652,\n",
       "    0.026783615742007588,\n",
       "    -0.003190381499049794,\n",
       "    0.010443712737247524,\n",
       "    0.006248606925847109,\n",
       "    0.0005256589578890179,\n",
       "    -0.020928088881341966,\n",
       "    -0.007326186741629102,\n",
       "    0.01051826231646687,\n",
       "    -0.003656316136340065,\n",
       "    -0.03223250853420404,\n",
       "    0.012585317899862537,\n",
       "    -0.00958300489031038,\n",
       "    -0.02544172517870452,\n",
       "    0.017254829658815685,\n",
       "    -0.020941643350290935,\n",
       "    0.004822000000290377,\n",
       "    -0.004781336593443461,\n",
       "    0.004364536673262567,\n",
       "    0.020209702027046438,\n",
       "    -0.007875142734062476,\n",
       "    -0.008315662043581482,\n",
       "    -0.001419829923912893,\n",
       "    -0.004564465090259906,\n",
       "    -0.007793815454707348,\n",
       "    -0.03074151881246228,\n",
       "    0.009779544690070476,\n",
       "    0.0038833532584047017,\n",
       "    0.011839822107669068,\n",
       "    0.1818466660124421,\n",
       "    0.013371476167580335,\n",
       "    -0.01568251312338009,\n",
       "    0.018678048898457762,\n",
       "    0.012605649603285996,\n",
       "    -0.011080772777849215,\n",
       "    0.004845720320951079,\n",
       "    -0.01232100575535758,\n",
       "    0.008085236071448955,\n",
       "    -0.011304421515507257,\n",
       "    0.01202958467295468,\n",
       "    0.0033259261885395155,\n",
       "    -0.01984373136542419,\n",
       "    0.0064587011945561774,\n",
       "    0.009644000000580754,\n",
       "    -0.008376657153851857,\n",
       "    -0.027461339189456195,\n",
       "    -0.04735928843067627,\n",
       "    -0.0015164053987589963,\n",
       "    0.009278029338958505,\n",
       "    0.003727477098322169,\n",
       "    0.0012148186974750127,\n",
       "    -0.010891010212563605,\n",
       "    -0.020074157337556717,\n",
       "    0.02992825253816913,\n",
       "    -0.001374083591210112,\n",
       "    0.0014587989057258646,\n",
       "    -0.011548401956588756,\n",
       "    0.009189925290790187,\n",
       "    0.008634192063882327,\n",
       "    -0.009447460200820658,\n",
       "    0.006085953298459444,\n",
       "    -0.0011495878156580841,\n",
       "    0.01098589149520641,\n",
       "    -0.017431037755152323,\n",
       "    -0.001968785683515619,\n",
       "    -0.005923299671071777,\n",
       "    -0.03708500841793608,\n",
       "    0.0229070413478919,\n",
       "    0.015601185378363669,\n",
       "    0.029087875463332854,\n",
       "    0.004120556232181068,\n",
       "    0.0022314032865713073,\n",
       "    -0.0022229317434781996,\n",
       "    0.0036292071984421206,\n",
       "    0.014055976849503429,\n",
       "    ...],\n",
       "   'text': 'Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4',\n",
       "   'metadata': {'source': 'sample_doc/doc.pdf', 'page': 3}}},\n",
       " {'_index': 'semantic-index',\n",
       "  '_id': 'doc0',\n",
       "  '_score': 1.7936683,\n",
       "  '_source': {'vector_field': [-0.03484686226964574,\n",
       "    0.011483308374112735,\n",
       "    0.03935940716637783,\n",
       "    -0.01642064441479642,\n",
       "    0.025557152057482067,\n",
       "    0.012332892524661162,\n",
       "    -0.009930381005978112,\n",
       "    -0.012200580452666187,\n",
       "    -0.009220073357908289,\n",
       "    -0.04039004974569356,\n",
       "    0.029693649373067052,\n",
       "    0.03033431923555888,\n",
       "    0.010048765148328444,\n",
       "    0.03306412825332625,\n",
       "    -0.009470769572253596,\n",
       "    0.006222156583925238,\n",
       "    0.03353766854801802,\n",
       "    -0.0006054154599669506,\n",
       "    -0.008913665424984406,\n",
       "    -0.012256290308599538,\n",
       "    -0.024721494905255676,\n",
       "    0.0025191552519044326,\n",
       "    -0.013885819892795784,\n",
       "    0.014345431326520301,\n",
       "    -0.004717975572562951,\n",
       "    0.022019539884132373,\n",
       "    0.02665743149105319,\n",
       "    -0.022855197036358764,\n",
       "    -0.026323168630162635,\n",
       "    -0.03055716052193751,\n",
       "    0.025292526050846895,\n",
       "    0.018788336575029166,\n",
       "    -0.011337068372473115,\n",
       "    -0.01803624513802541,\n",
       "    -0.01598888884036119,\n",
       "    -0.012611444598295756,\n",
       "    0.011281358516539764,\n",
       "    -0.005894858153518308,\n",
       "    0.03295270854145954,\n",
       "    -0.002801189075119535,\n",
       "    0.031559949104609186,\n",
       "    0.017785547992357494,\n",
       "    -0.00030814822563750364,\n",
       "    -0.012750720169451748,\n",
       "    -0.005915749116662663,\n",
       "    0.004940817324602888,\n",
       "    0.009784141004338493,\n",
       "    -0.028913703939419232,\n",
       "    -0.001063720713729808,\n",
       "    0.01959613786796627,\n",
       "    0.029359386512176493,\n",
       "    0.02353764975646191,\n",
       "    -0.0010332540079164452,\n",
       "    -0.024735421903577706,\n",
       "    -0.03395549712413122,\n",
       "    -0.011622584876591338,\n",
       "    0.009415059716320243,\n",
       "    0.021378870021640545,\n",
       "    0.0054944392566302624,\n",
       "    -0.0038858011012396767,\n",
       "    0.023649069468328617,\n",
       "    -0.003659477599619232,\n",
       "    -0.028314816934538724,\n",
       "    0.006545973264751661,\n",
       "    -0.012117014737443546,\n",
       "    -0.012506987454267456,\n",
       "    -0.0032016077391769275,\n",
       "    0.010696399441303898,\n",
       "    0.0020995860338316346,\n",
       "    0.0039032107804648285,\n",
       "    0.03913656401735397,\n",
       "    0.012242363310277506,\n",
       "    0.0026079435914978344,\n",
       "    -0.007778566167301675,\n",
       "    0.03724241028916778,\n",
       "    -0.033732653975107364,\n",
       "    -0.011107263586933468,\n",
       "    -0.016880253985875716,\n",
       "    -0.018495856571749927,\n",
       "    0.005146249397417673,\n",
       "    0.014582200542543576,\n",
       "    -0.0016225657357892508,\n",
       "    -0.005386500363021456,\n",
       "    0.02902512365128594,\n",
       "    0.007966588560891307,\n",
       "    0.0052959711486378,\n",
       "    0.010278570865190701,\n",
       "    -0.0008482780870020936,\n",
       "    -0.03245131425012371,\n",
       "    0.015250725333002079,\n",
       "    0.016740978414719724,\n",
       "    0.011963811236642911,\n",
       "    0.025181106338980193,\n",
       "    0.01855156642768328,\n",
       "    -0.001468491681178162,\n",
       "    0.022938762751581403,\n",
       "    -0.0272563184959337,\n",
       "    0.04615607664547479,\n",
       "    -0.0007938733632924632,\n",
       "    -0.017660201282168757,\n",
       "    0.009352385429903264,\n",
       "    -0.009923417506817096,\n",
       "    -0.016392788555507132,\n",
       "    -0.012242363310277506,\n",
       "    -0.005153212896578689,\n",
       "    -0.01360726781916119,\n",
       "    0.008676896208961131,\n",
       "    -0.007493050594506064,\n",
       "    -0.003805717601258851,\n",
       "    0.0002372044929285013,\n",
       "    -0.01461005547051025,\n",
       "    0.026225675916617962,\n",
       "    0.003805717601258851,\n",
       "    -0.054568348710445976,\n",
       "    -0.0002552668546808403,\n",
       "    -0.006114217690316431,\n",
       "    0.004052932531684955,\n",
       "    -0.009839851791594457,\n",
       "    -0.015807828548948657,\n",
       "    -0.027117042924777707,\n",
       "    0.048690898373507596,\n",
       "    0.0163788615571851,\n",
       "    -0.0017984017730164525,\n",
       "    -0.02491648219499024,\n",
       "    0.024498653618877042,\n",
       "    0.035654661699937626,\n",
       "    -0.014373286254486977,\n",
       "    -0.01213094173576558,\n",
       "    -0.007493050594506064,\n",
       "    -0.024777204761189026,\n",
       "    0.0005157565665630956,\n",
       "    0.014582200542543576,\n",
       "    -0.013314788747204563,\n",
       "    0.020947114447205314,\n",
       "    -0.004181762854099423,\n",
       "    0.02465185805100029,\n",
       "    -0.00040629429109748956,\n",
       "    -0.03345410283279538,\n",
       "    -0.023078038322737394,\n",
       "    -0.017493069851723476,\n",
       "    -0.003313028615196896,\n",
       "    0.006928982482414554,\n",
       "    -0.015013956116978805,\n",
       "    -0.01642064441479642,\n",
       "    0.010668543582014611,\n",
       "    0.03487471626628981,\n",
       "    -0.0022092660350613485,\n",
       "    0.03860731572937407,\n",
       "    0.013119802388792607,\n",
       "    0.01276464809909639,\n",
       "    0.009442914644286919,\n",
       "    -0.01910867057495247,\n",
       "    0.013140692886275658,\n",
       "    -0.0022823860358811583,\n",
       "    0.018495856571749927,\n",
       "    0.02378834503948461,\n",
       "    -0.017576635566946115,\n",
       "    0.028579441078528674,\n",
       "    -0.014568272612898931,\n",
       "    0.0004583051955551942,\n",
       "    0.007931770133763616,\n",
       "    0.001502440136572033,\n",
       "    -0.010717290870109558,\n",
       "    -0.0009026827525040608,\n",
       "    0.03791093601094889,\n",
       "    0.028830138224196594,\n",
       "    0.02426188347153116,\n",
       "    -0.007451267736894745,\n",
       "    0.002672358752705068,\n",
       "    -0.023328735468405314,\n",
       "    -0.021323160165707192,\n",
       "    0.03089142338282807,\n",
       "    -0.0351811251305363,\n",
       "    0.021351016024996478,\n",
       "    -0.01590532312513855,\n",
       "    0.0014136516805633049,\n",
       "    0.04821736180410627,\n",
       "    -0.012214507450988218,\n",
       "    -0.034261902263087264,\n",
       "    0.004474242391717355,\n",
       "    0.02378834503948461,\n",
       "    0.001082000597519434,\n",
       "    0.02955437380191106,\n",
       "    0.017562706705978863,\n",
       "    -0.010494448652408317,\n",
       "    0.009477734002737222,\n",
       "    0.01590532312513855,\n",
       "    -0.026100327343784004,\n",
       "    -0.009923417506817096,\n",
       "    -0.019805052156022872,\n",
       "    0.019832906152666935,\n",
       "    0.03264629967721305,\n",
       "    3.786567105451108e-05,\n",
       "    -0.029526517942621774,\n",
       "    -0.605683588677925,\n",
       "    -0.02218667131457765,\n",
       "    0.018217305429437943,\n",
       "    -0.019415077576553738,\n",
       "    -0.0028273033611266097,\n",
       "    0.016490281269051804,\n",
       "    -0.014888607544144845,\n",
       "    -0.014916462472111522,\n",
       "    -0.015752118693015304,\n",
       "    0.030222897661046955,\n",
       "    -0.021337087164029222,\n",
       "    -0.021504218594474503,\n",
       "    -0.0033199923471885647,\n",
       "    -0.007611435202517702,\n",
       "    -0.02481898761880035,\n",
       "    -0.022980545609192726,\n",
       "    0.004798059072543777,\n",
       "    -0.017604489563590182,\n",
       "    0.032312040541612935,\n",
       "    0.026587794636797807,\n",
       "    -0.027019550211233034,\n",
       "    0.03011147794918025,\n",
       "    -0.0076462540953067,\n",
       "    0.005483993542227433,\n",
       "    0.01083567501245989,\n",
       "    0.006685249301568958,\n",
       "    0.02413653676134242,\n",
       "    -0.02189419131129841,\n",
       "    0.002621871288311825,\n",
       "    -0.0014597868415655861,\n",
       "    -0.03980508973913509,\n",
       "    0.0022179708746739245,\n",
       "    0.017869113707580132,\n",
       "    0.005323826076604476,\n",
       "    0.03562680770329356,\n",
       "    -0.0007224944118858974,\n",
       "    -0.00849583778019382,\n",
       "    0.017715911138102107,\n",
       "    -0.010376064510057985,\n",
       "    0.028036263929581518,\n",
       "    -0.0311142646692067,\n",
       "    -0.03949868087488859,\n",
       "    0.03041788495078152,\n",
       "    0.023649069468328617,\n",
       "    0.0032520952035701703,\n",
       "    0.014763258971310887,\n",
       "    0.012346819522983193,\n",
       "    0.012040412521381924,\n",
       "    -0.005442210684616113,\n",
       "    -0.01959613786796627,\n",
       "    -0.025974978770950043,\n",
       "    -0.013022308743925326,\n",
       "    -0.0017949197906052916,\n",
       "    -0.017339865419600232,\n",
       "    0.01569640883708195,\n",
       "    0.007569652344906382,\n",
       "    0.046796744645321395,\n",
       "    -0.005038310503808865,\n",
       "    0.030139331945824313,\n",
       "    0.004136498246907595,\n",
       "    -0.02240951260095628,\n",
       "    0.02850980236162807,\n",
       "    -0.004272292068483078,\n",
       "    -0.027284174355222988,\n",
       "    -0.007541796951278401,\n",
       "    -0.0070125481976371935,\n",
       "    0.003140674560380855,\n",
       "    0.023774418041162575,\n",
       "    0.010773000726042911,\n",
       "    -0.02582177620147202,\n",
       "    0.02033430044400277,\n",
       "    0.026490300060607912,\n",
       "    -0.010111439434745424,\n",
       "    -0.005194995754190009,\n",
       "    -0.0030832231311652904,\n",
       "    0.030083622089890964,\n",
       "    0.003965885066881808,\n",
       "    0.020598924587992725,\n",
       "    0.009143471141846665,\n",
       "    0.01910867057495247,\n",
       "    -0.0011211720265300463,\n",
       "    -0.011991666164609588,\n",
       "    -0.002484336126284782,\n",
       "    -0.01694989270277632,\n",
       "    0.012841250315158015,\n",
       "    0.01018107815164603,\n",
       "    -0.0030449222559651313,\n",
       "    0.002345060089467485,\n",
       "    -0.014150444968108345,\n",
       "    -0.008133719991336587,\n",
       "    0.0165877758452417,\n",
       "    -0.006068953083124603,\n",
       "    -0.030306463376269594,\n",
       "    -0.0313649599522294,\n",
       "    0.010090548005939764,\n",
       "    0.015891394264171296,\n",
       "    -0.0011951624647449828,\n",
       "    0.009240964786713948,\n",
       "    0.010543195009180653,\n",
       "    -0.0011794938931407378,\n",
       "    -0.0035793938668077533,\n",
       "    -0.015069665972912157,\n",
       "    0.02405297104611978,\n",
       "    0.005504884971033093,\n",
       "    0.028607296937817963,\n",
       "    0.006448480085545683,\n",
       "    -0.007207534556049148,\n",
       "    0.011685259163008317,\n",
       "    0.019164382293531044,\n",
       "    -0.021072463020039272,\n",
       "    -0.013509775105616517,\n",
       "    -0.00784123998805735,\n",
       "    -0.015626770120181346,\n",
       "    0.019261875007075716,\n",
       "    0.01473540404334421,\n",
       "    -0.037325976004390415,\n",
       "    0.02037608330161409,\n",
       "    0.007855167917701992,\n",
       "    -0.015835684408237943,\n",
       "    -0.027339884211156337,\n",
       "    0.0014728439845691237,\n",
       "    -0.006636502944796622,\n",
       "    -0.005024382574164222,\n",
       "    -0.0014258383861717153,\n",
       "    0.007103077412020849,\n",
       "    0.009874670218722148,\n",
       "    -0.0007216239162831071,\n",
       "    -0.019401150578231708,\n",
       "    -0.011309213444506439,\n",
       "    0.03189421196549974,\n",
       "    0.018997250863085763,\n",
       "    -0.010410882937185678,\n",
       "    0.04203350632821184,\n",
       "    0.00025374350192787325,\n",
       "    0.006845416767191915,\n",
       "    0.026420663206352526,\n",
       "    -0.0006167316699725707,\n",
       "    -0.024624002191711004,\n",
       "    0.020000037583112217,\n",
       "    -0.010188041650807046,\n",
       "    0.008878846997856715,\n",
       "    0.004209617782066099,\n",
       "    0.028133758505771413,\n",
       "    -0.0086629692106391,\n",
       "    -0.03810592143803823,\n",
       "    0.0015033106903824863,\n",
       "    -0.016281366980995207,\n",
       "    0.0013518479479567788,\n",
       "    -0.001946382533030999,\n",
       "    -0.011692222662169334,\n",
       "    -0.0009566521993084645,\n",
       "    -0.004387194926914207,\n",
       "    -0.011678294732524689,\n",
       "    0.0038440184764590097,\n",
       "    -0.0051671408262233326,\n",
       "    -0.026030690489528618,\n",
       "    -0.0217131328825311,\n",
       "    -0.040975006026961594,\n",
       "    -0.022200598312899684,\n",
       "    -0.03047359480671487,\n",
       "    0.021086390018361306,\n",
       "    0.020543214732059372,\n",
       "    -0.022938762751581403,\n",
       "    0.008321742850587526,\n",
       "    0.00011577320123880351,\n",
       "    -0.010870494370910195,\n",
       "    -0.015278580260968755,\n",
       "    0.03125354024036269,\n",
       "    -0.03376050797175143,\n",
       "    -0.017186662850122207,\n",
       "    0.017339865419600232,\n",
       "    -0.039916509451001794,\n",
       "    -0.009435951145125902,\n",
       "    0.0010602387313186472,\n",
       "    -0.006511154371962663,\n",
       "    0.02040393916090338,\n",
       "    -0.01319640367353162,\n",
       "    0.0053342717910073055,\n",
       "    -0.008892773996178747,\n",
       "    -0.010083584506778747,\n",
       "    0.01837050799891597,\n",
       "    -0.00012980960813973252,\n",
       "    -0.03000005637466832,\n",
       "    -0.008043190776952931,\n",
       "    0.018078027995636733,\n",
       "    0.024860770476411668,\n",
       "    0.023983332329219176,\n",
       "    -0.005059201932614525,\n",
       "    -0.012193616022182559,\n",
       "    0.012597516668651111,\n",
       "    0.003659477599619232,\n",
       "    0.009798068933983136,\n",
       "    -0.011420634087695756,\n",
       "    0.011559910590174359,\n",
       "    -0.008593331425061104,\n",
       "    0.007569652344906382,\n",
       "    0.03621176770985204,\n",
       "    -0.02590534191669466,\n",
       "    -0.0152925081906134,\n",
       "    0.030195043664402888,\n",
       "    0.010222860077934739,\n",
       "    0.00925489178503598,\n",
       "    0.0019272320954309198,\n",
       "    -0.029164401085087152,\n",
       "    0.009289711143486284,\n",
       "    0.0036873327604165606,\n",
       "    0.012388602380594513,\n",
       "    -0.02742344992637898,\n",
       "    -0.0014771964043754115,\n",
       "    0.015097521832201444,\n",
       "    0.02717275278071106,\n",
       "    -0.018356581000593935,\n",
       "    -0.01707524127561028,\n",
       "    0.0017078723258021442,\n",
       "    -0.012750720169451748,\n",
       "    0.05175497211481074,\n",
       "    0.012158797595054865,\n",
       "    0.024331522188431765,\n",
       "    -0.01165740330371903,\n",
       "    -0.0012029967505471052,\n",
       "    0.01846800071246064,\n",
       "    -0.013307824316720935,\n",
       "    0.011337068372473115,\n",
       "    0.005957531974273982,\n",
       "    -0.004634409857340312,\n",
       "    0.01768805527881282,\n",
       "    0.015571060264247993,\n",
       "    0.014623983400154895,\n",
       "    0.011253503588573088,\n",
       "    -0.016058525694616577,\n",
       "    -0.0077716022024793534,\n",
       "    0.02643459020467456,\n",
       "    0.010383028009219001,\n",
       "    0.010132330863551083,\n",
       "    0.008001407919341612,\n",
       "    0.003330438061591395,\n",
       "    0.025208960335624257,\n",
       "    -0.005125357968612013,\n",
       "    0.01516715961777944,\n",
       "    -0.011448489946985042,\n",
       "    0.013426209390393878,\n",
       "    0.011566874089335374,\n",
       "    -0.004926889394958244,\n",
       "    -0.03403906283935386,\n",
       "    0.014860752616178169,\n",
       "    0.0156824799761147,\n",
       "    0.04008364088144707,\n",
       "    0.006699176765552295,\n",
       "    -0.033119839971904826,\n",
       "    0.01003483815000641,\n",
       "    -0.02111424587765059,\n",
       "    0.028690862653040602,\n",
       "    -0.016114237413195148,\n",
       "    0.010773000726042911,\n",
       "    0.010828711513298873,\n",
       "    0.009387204788353566,\n",
       "    -0.00215007373105553,\n",
       "    0.0014258383861717153,\n",
       "    0.02825910707860537,\n",
       "    0.034289759985021775,\n",
       "    -0.0067653332672110884,\n",
       "    0.00375000704683354,\n",
       "    0.00896937621224037,\n",
       "    -0.007451267736894745,\n",
       "    0.007200570591226827,\n",
       "    -0.0112256477292838,\n",
       "    -0.0014771964043754115,\n",
       "    -0.026935984496010396,\n",
       "    -0.009185253999457984,\n",
       "    0.00426881031890257,\n",
       "    -0.024122607900375168,\n",
       "    -0.012674118884712735,\n",
       "    -0.002134405159451285,\n",
       "    -0.03206134339594502,\n",
       "    0.015264652331324112,\n",
       "    0.018412290856527288,\n",
       "    -0.032479168246767776,\n",
       "    0.018746553717417846,\n",
       "    0.006723550409599769,\n",
       "    0.05771598816697176,\n",
       "    -0.019791123295055616,\n",
       "    -0.050390066674604446,\n",
       "    0.019972181723822927,\n",
       "    0.001240427188352137,\n",
       "    -0.006525081835946001,\n",
       "    -0.036239621706496104,\n",
       "    -0.01850978357007196,\n",
       "    -0.015403928833802715,\n",
       "    -0.030696436093093502,\n",
       "    -0.013509775105616517,\n",
       "    -0.00977021307469385,\n",
       "    0.016281366980995207,\n",
       "    0.00426881031890257,\n",
       "    0.0010010464273128286,\n",
       "    -0.019526499151065666,\n",
       "    -0.0007747229256923838,\n",
       "    0.0327577231143702,\n",
       "    -0.016573846984274443,\n",
       "    -0.006263939441536558,\n",
       "    -0.028091975648160093,\n",
       "    0.015473566619380711,\n",
       "    0.0028708270935281837,\n",
       "    -0.04242347718239053,\n",
       "    -0.006316168013550708,\n",
       "    0.009240964786713948,\n",
       "    -0.013467992248005198,\n",
       "    -0.006476335479173665,\n",
       "    -0.019192236290175108,\n",
       "    -0.020459649016836733,\n",
       "    -0.014442924040064973,\n",
       "    0.03011147794918025,\n",
       "    0.003318251472398311,\n",
       "    -0.004080787459651632,\n",
       "    -0.021253521448806583,\n",
       "    0.023259096751504706,\n",
       "    -0.004160871425293763,\n",
       "    -0.019818979154344905,\n",
       "    -0.003457527509215608,\n",
       "    0.017089168273932313,\n",
       "    0.007688036952918019,\n",
       "    -0.011664367734202657,\n",
       "    -0.029582227798555127,\n",
       "    0.0011969033395352368,\n",
       "    0.010313390223641005,\n",
       "    0.05860735331248629,\n",
       "    0.03738168772296899,\n",
       "    -0.03924798372922068,\n",
       "    0.020348227442324805,\n",
       "    -0.02616996606068461,\n",
       "    0.008530657138644124,\n",
       "    -0.023175531036282067,\n",
       "    -0.030696436093093502,\n",
       "    0.014707549115377534,\n",
       "    -0.010389991508380018,\n",
       "    -0.004112124602860121,\n",
       "    -0.0084540549225825,\n",
       "    0.013036236673569969,\n",
       "    0.0011821053217414453,\n",
       "    0.03554324198807092,\n",
       "    0.01174096901894167,\n",
       "    -0.011232612159767428,\n",
       "    -0.007548760916100722,\n",
       "    0.0038962468156425066,\n",
       "    -0.004944299074183396,\n",
       "    -0.001219535759546477,\n",
       "    0.00433148460531955,\n",
       "    0.0361282019946294,\n",
       "    0.011128155015739128,\n",
       "    0.026880272777431824,\n",
       "    -0.013600304320000173,\n",
       "    0.027855205500814207,\n",
       "    0.024637929190033034,\n",
       "    0.007506978058489403,\n",
       "    -0.015376073905836039,\n",
       "    -0.002017761426229902,\n",
       "    0.007437339807250101,\n",
       "    -0.003560243545623,\n",
       "    0.01824515942608201,\n",
       "    -0.0059018216526793245,\n",
       "    0.03632318742171874,\n",
       "    0.004519506998909183,\n",
       "    0.020292517586391452,\n",
       "    0.015612843121859314,\n",
       "    0.010389991508380018,\n",
       "    0.026114256204751257,\n",
       "    0.0318106462502771,\n",
       "    0.00011370581783792413,\n",
       "    -0.009582191146765522,\n",
       "    0.002983988844338406,\n",
       "    -0.023593359612395264,\n",
       "    -0.01005572957881207,\n",
       "    0.010327317221963037,\n",
       "    -0.022952689749903436,\n",
       "    -0.02456829233577765,\n",
       "    -0.0015659848603841395,\n",
       "    -0.019512572152743633,\n",
       "    -0.05426193984619948,\n",
       "    0.00623956626315039,\n",
       "    0.016935965704454287,\n",
       "    0.004028559353298787,\n",
       "    -0.0006062859555697409,\n",
       "    -0.0063475051567591975,\n",
       "    -0.010174113721162403,\n",
       "    -0.0001233898558642705,\n",
       "    -0.010215896578773722,\n",
       "    -0.025222889196591512,\n",
       "    0.020250734728780133,\n",
       "    0.005379536398199134,\n",
       "    0.022256310031478255,\n",
       "    -0.020292517586391452,\n",
       "    -0.02552929619819278,\n",
       "    -0.01174096901894167,\n",
       "    -0.030055766230601674,\n",
       "    0.008022299348147272,\n",
       "    0.008426199994615824,\n",
       "    -0.035320402564337515,\n",
       "    -0.057827407878838465,\n",
       "    0.005431764970213283,\n",
       "    0.030919279242117355,\n",
       "    0.02296661674822547,\n",
       "    0.015431783761769392,\n",
       "    -0.003624658706830234,\n",
       "    -0.009658792431504535,\n",
       "    0.007444303772072423,\n",
       "    -0.008718679066572452,\n",
       "    -0.02694991149433243,\n",
       "    -0.007235389484015825,\n",
       "    -0.029136545225797863,\n",
       "    -0.008022299348147272,\n",
       "    -0.010494448652408317,\n",
       "    -0.009860743220400116,\n",
       "    0.00029465583483257113,\n",
       "    -0.013725652892834133,\n",
       "    -0.006545973264751661,\n",
       "    0.021657423026597747,\n",
       "    0.0002515673211285609,\n",
       "    0.030139331945824313,\n",
       "    -0.002849935664722524,\n",
       "    0.009895561647527808,\n",
       "    0.03924798372922068,\n",
       "    0.02174098874182039,\n",
       "    0.013823145606378805,\n",
       "    -0.0022701992138574214,\n",
       "    -0.029192255081731216,\n",
       "    -0.006622575015151978,\n",
       "    -0.023857983756385214,\n",
       "    0.007082185983215189,\n",
       "    -0.032479168246767776,\n",
       "    0.0042444371405164025,\n",
       "    0.010334281652446665,\n",
       "    0.02284126817539151,\n",
       "    0.013140692886275658,\n",
       "    0.002489558983486197,\n",
       "    -0.01945686043416506,\n",
       "    -0.010348208650768697,\n",
       "    -0.018300871144660585,\n",
       "    -0.01016018672284037,\n",
       "    -0.004801541287785591,\n",
       "    0.008871883498695698,\n",
       "    0.003154602024364193,\n",
       "    0.026142110201395324,\n",
       "    -0.011309213444506439,\n",
       "    -0.016114237413195148,\n",
       "    -0.0027802976463138754,\n",
       "    -0.0013431432247595293,\n",
       "    -0.016406715553829165,\n",
       "    0.02717275278071106,\n",
       "    -0.010731217868431592,\n",
       "    -0.0007303286976880198,\n",
       "    0.0173259384212782,\n",
       "    -0.007437339807250101,\n",
       "    -0.007388593450477765,\n",
       "    -0.01933151372397632,\n",
       "    0.03080785766760543,\n",
       "    0.016183874267450535,\n",
       "    0.04615607664547479,\n",
       "    0.012771611598257408,\n",
       "    -0.023676925327617903,\n",
       "    -0.0305850163812268,\n",
       "    -0.009331494001097604,\n",
       "    -0.020083603298334855,\n",
       "    -0.020222878869490847,\n",
       "    -0.0018367026482166114,\n",
       "    -0.015013956116978805,\n",
       "    -0.0030832231311652904,\n",
       "    -0.0051392854325953505,\n",
       "    -0.01725629970437759,\n",
       "    0.006107253725494109,\n",
       "    0.007743747274512677,\n",
       "    -0.0296379395171337,\n",
       "    -0.011650439804558014,\n",
       "    0.01685239998923165,\n",
       "    0.015027883115300837,\n",
       "    0.00383357276205618,\n",
       "    -0.004878143038185908,\n",
       "    -0.004676192714951632,\n",
       "    -0.018300871144660585,\n",
       "    -0.006733996124002599,\n",
       "    0.01481896975856685,\n",
       "    -0.025292526050846895,\n",
       "    -0.0034331540979987874,\n",
       "    0.009310602572291944,\n",
       "    0.031532091382674675,\n",
       "    0.028913703939419232,\n",
       "    0.04337055404648363,\n",
       "    0.011448489946985042,\n",
       "    -0.0008796151720029202,\n",
       "    0.039526538596823105,\n",
       "    -0.018662988002195208,\n",
       "    0.003476677830400361,\n",
       "    0.0032903960787703294,\n",
       "    0.028635150934462027,\n",
       "    -0.021211738591195264,\n",
       "    0.0053621271846352875,\n",
       "    0.00019019882177151405,\n",
       "    0.007889987276152297,\n",
       "    -0.004105161103699105,\n",
       "    -0.004784131608560439,\n",
       "    0.0331755516904834,\n",
       "    0.0029526517011299164,\n",
       "    -0.005626751794286544,\n",
       "    -0.018704770859806527,\n",
       "    -0.01664348570117505,\n",
       "    -0.014498634827320936,\n",
       "    -0.00391017474528715,\n",
       "    0.011545982660529714,\n",
       "    -0.0011028920263250939,\n",
       "    -0.003659477599619232,\n",
       "    -0.015919250123460585,\n",
       "    0.0012726347689557538,\n",
       "    0.035431822276204214,\n",
       "    0.0061873376911362405,\n",
       "    0.016044598696294543,\n",
       "    0.016448498411440485,\n",
       "    0.04242347718239053,\n",
       "    0.00378134419004203,\n",
       "    -0.004707529858160121,\n",
       "    0.006295276584745048,\n",
       "    0.028147685504093446,\n",
       "    -0.006093326261510771,\n",
       "    -0.014025096395274387,\n",
       "    -0.02816161250241548,\n",
       "    -0.011497236303757378,\n",
       "    0.0282033953600268,\n",
       "    0.006946392161639706,\n",
       "    -0.00010315130603666123,\n",
       "    -0.00313545170317944,\n",
       "    0.004007667924493127,\n",
       "    0.008440126992937858,\n",
       "    0.0025835704131116663,\n",
       "    -0.004975636217391886,\n",
       "    -0.028495875363306035,\n",
       "    -0.016309222840284493,\n",
       "    -0.01871869785812856,\n",
       "    -0.015473566619380711,\n",
       "    -0.02590534191669466,\n",
       "    -0.02980507094757898,\n",
       "    -0.007137896304809847,\n",
       "    -0.01124653915808946,\n",
       "    -0.0008857085248071254,\n",
       "    -0.024679712047644357,\n",
       "    0.006232602298328068,\n",
       "    -0.018384434997238002,\n",
       "    -0.03456831112733376,\n",
       "    0.03000005637466832,\n",
       "    0.018927612146185158,\n",
       "    0.041086429464118744,\n",
       "    -0.004481206356539677,\n",
       "    0.015236797403357436,\n",
       "    0.027493088643279585,\n",
       "    0.011518127732563038,\n",
       "    0.0007525257825863703,\n",
       "    0.002959615665952238,\n",
       "    -0.017576635566946115,\n",
       "    0.006633020729554808,\n",
       "    0.019526499151065666,\n",
       "    0.009610046074732198,\n",
       "    -0.0224512954585676,\n",
       "    -0.009700575289115854,\n",
       "    -0.01202648552305989,\n",
       "    0.01650421013001906,\n",
       "    -0.025557152057482067,\n",
       "    -0.036295333425074676,\n",
       "    0.029359386512176493,\n",
       "    0.008746534925861738,\n",
       "    0.017674128280490788,\n",
       "    -0.016267439982673174,\n",
       "    -0.02089140459127196,\n",
       "    -0.0038649099052646694,\n",
       "    -0.0027924844683376118,\n",
       "    -0.026768853065565118,\n",
       "    0.025069684764468265,\n",
       "    0.0075766158440673985,\n",
       "    -0.019415077576553738,\n",
       "    -0.0031424154351711087,\n",
       "    0.010598905796436615,\n",
       "    -0.00038409717709530757,\n",
       "    0.015529277406636673,\n",
       "    0.012291109667049842,\n",
       "    -0.018857973429284552,\n",
       "    0.021281377308095872,\n",
       "    0.004986081931794716,\n",
       "    -0.0156824799761147,\n",
       "    -9.428334672399383e-05,\n",
       "    0.007987479989696967,\n",
       "    0.0052019597190123305,\n",
       "    -0.033259117405706036,\n",
       "    0.04693602207912261,\n",
       "    0.01477718690095553,\n",
       "    -0.006072434832705111,\n",
       "    0.01461005547051025,\n",
       "    -0.007980516490535952,\n",
       "    -0.004885106537346925,\n",
       "    0.035320402564337515,\n",
       "    -0.014763258971310887,\n",
       "    -0.00665043040877996,\n",
       "    0.0022214526242544325,\n",
       "    -0.016838471128264396,\n",
       "    0.02491648219499024,\n",
       "    -0.013871892894473752,\n",
       "    -0.004724939071723968,\n",
       "    -0.006539009765590644,\n",
       "    -0.009721466717921514,\n",
       "    -0.0006663487551783497,\n",
       "    0.009240964786713948,\n",
       "    0.010731217868431592,\n",
       "    -0.010494448652408317,\n",
       "    -0.017186662850122207,\n",
       "    0.0009409836859118828,\n",
       "    0.005578004971852902,\n",
       "    0.005717281008670199,\n",
       "    0.015195014545746116,\n",
       "    0.017520923848367544,\n",
       "    -0.03331482539899417,\n",
       "    0.004568253821342824,\n",
       "    0.014721476113699568,\n",
       "    -0.005616306079883714,\n",
       "    0.02144850873854115,\n",
       "    0.009247928285874963,\n",
       "    -0.012876068742285706,\n",
       "    -0.0026027207342964194,\n",
       "    0.021072463020039272,\n",
       "    -0.014115625609658042,\n",
       "    -0.008795281282634074,\n",
       "    0.038830155153107476,\n",
       "    -0.008732606996217095,\n",
       "    0.0024199209650775486,\n",
       "    0.007569652344906382,\n",
       "    0.0010228082935136154,\n",
       "    -0.00563371529344756,\n",
       "    0.014415069112098297,\n",
       "    -0.004919925895797228,\n",
       "    -0.015863540267527232,\n",
       "    0.009930381005978112,\n",
       "    -0.009199181929102629,\n",
       "    0.016448498411440485,\n",
       "    0.004401122390897546,\n",
       "    -0.014157408467269362,\n",
       "    -0.013495847175971874,\n",
       "    0.01096102358529385,\n",
       "    0.009268819714680625,\n",
       "    -0.011455453446146058,\n",
       "    0.0015120154135797358,\n",
       "    -0.008119792061691944,\n",
       "    0.004171317139696593,\n",
       "    0.0037256338684473725,\n",
       "    0.014916462472111522,\n",
       "    -0.012465204596656137,\n",
       "    0.016058525694616577,\n",
       "    0.016225657125061854,\n",
       "    -0.007026475661620532,\n",
       "    0.007959625061730292,\n",
       "    -0.03389978540555264,\n",
       "    0.00038105050069320496,\n",
       "    -0.0163788615571851,\n",
       "    0.013676905604739185,\n",
       "    -0.012054340451026567,\n",
       "    -0.012360747452627838,\n",
       "    0.019679703583188914,\n",
       "    0.006232602298328068,\n",
       "    -0.003805717601258851,\n",
       "    0.019052960719019116,\n",
       "    0.01603067169797251,\n",
       "    -0.009178290500296967,\n",
       "    -0.015306435188935432,\n",
       "    0.014143480537624719,\n",
       "    0.022548790034757495,\n",
       "    0.008057118706597574,\n",
       "    -0.03142067167080797,\n",
       "    0.017409504136500838,\n",
       "    0.0133635351039769,\n",
       "    -0.0019603099970143373,\n",
       "    -0.02988863666280162,\n",
       "    -0.022618426889012878,\n",
       "    -0.0012682823491494658,\n",
       "    0.011121190585255502,\n",
       "    0.004039005067701617,\n",
       "    -0.010821747082815247,\n",
       "    -0.017089168273932313,\n",
       "    -0.03175493453169853,\n",
       "    -0.04376052862595276,\n",
       "    0.004543880642956656,\n",
       "    -0.022214527173866936,\n",
       "    0.014846824686533526,\n",
       "    0.0005074870038557465,\n",
       "    0.0009340198957125506,\n",
       "    0.0025870521626921742,\n",
       "    0.014359358324842333,\n",
       "    -0.015403928833802715,\n",
       "    0.008830100641084379,\n",
       "    -0.014665766257766215,\n",
       "    0.008356561277715217,\n",
       "    0.005877448474293156,\n",
       "    -0.00028638633033288514,\n",
       "    -0.008955448282595726,\n",
       "    0.009582191146765522,\n",
       "    -0.03930369544779925,\n",
       "    -0.018913685147863124,\n",
       "    0.008112828562530927,\n",
       "    -0.014275792609619694,\n",
       "    -0.009617009573893213,\n",
       "    0.0165877758452417,\n",
       "    -0.001580782878177931,\n",
       "    0.017298082561988913,\n",
       "    0.005988869117482472,\n",
       "    0.03635104514365325,\n",
       "    0.010348208650768697,\n",
       "    0.020905331589593994,\n",
       "    0.026852418780787757,\n",
       "    -0.031225686243718628,\n",
       "    -0.014805041828922207,\n",
       "    -0.002111772855855371,\n",
       "    0.0010471815883151096,\n",
       "    -0.0032329448823854175,\n",
       "    -0.01906688771734115,\n",
       "    -0.0005449174416607783,\n",
       "    0.0030396993987637164,\n",
       "    -0.011323141374151083,\n",
       "    -0.034289759985021775,\n",
       "    0.01317551224472596,\n",
       "    0.018913685147863124,\n",
       "    -0.0057068352942673695,\n",
       "    0.02552929619819278,\n",
       "    0.009610046074732198,\n",
       "    0.0007738524300895936,\n",
       "    0.038802301156463416,\n",
       "    0.010717290870109558,\n",
       "    -0.010299462293996363,\n",
       "    -0.01935936772062039,\n",
       "    -0.0003721281390433421,\n",
       "    0.008238177135364885,\n",
       "    -0.02119781159287323,\n",
       "    0.030612870377870863,\n",
       "    -0.024108680902053134,\n",
       "    0.010459630225280623,\n",
       "    0.014345431326520301,\n",
       "    0.006142073083944413,\n",
       "    -0.014254901180814034,\n",
       "    -0.00868386063944476,\n",
       "    0.004230509210871759,\n",
       "    -0.00629179483516454,\n",
       "    -0.02661564863344187,\n",
       "    0.028635150934462027,\n",
       "    0.006392769763951025,\n",
       "    -0.043732674629308695,\n",
       "    0.0217131328825311,\n",
       "    -0.007806421560929657,\n",
       "    0.00019487762649714302,\n",
       "    -0.019944327727178864,\n",
       "    -0.004759758430174271,\n",
       "    0.0011612138929357856,\n",
       "    -0.015320363118580075,\n",
       "    0.014185263395236038,\n",
       "    0.008656004780155472,\n",
       "    -0.022019539884132373,\n",
       "    -0.010480520722763674,\n",
       "    -0.012256290308599538,\n",
       "    -0.0002155514168501895,\n",
       "    -0.005024382574164222,\n",
       "    -0.013105874459147964,\n",
       "    0.20501431129389963,\n",
       "    -0.03147638338938655,\n",
       "    0.00660864755116864,\n",
       "    0.017339865419600232,\n",
       "    -0.01451256182564297,\n",
       "    -0.02378834503948461,\n",
       "    0.010215896578773722,\n",
       "    0.021476364597830436,\n",
       "    -0.005400427827004794,\n",
       "    -0.016657412699497085,\n",
       "    0.006908091053608894,\n",
       "    -0.007862131416863009,\n",
       "    -0.041615677752098644,\n",
       "    -0.007952661562569276,\n",
       "    0.01520894247539076,\n",
       "    -0.007486086629683743,\n",
       "    -0.051922103545256025,\n",
       "    -0.05147642097249876,\n",
       "    -0.016100308552227896,\n",
       "    0.012604480167812128,\n",
       "    0.0018837082466140196,\n",
       "    -0.02310589231938146,\n",
       "    0.011427598518179382,\n",
       "    -0.02830088993621669,\n",
       "    0.0248329164797676,\n",
       "    -0.005793882759070517,\n",
       "    -0.006194301190297256,\n",
       "    -0.009561299717959862,\n",
       "    0.00659820183676581,\n",
       "    0.024679712047644357,\n",
       "    -0.03431761398166584,\n",
       "    0.006685249301568958,\n",
       "    0.0024582218402777072,\n",
       "    -0.0013927602517576452,\n",
       "    -0.03456831112733376,\n",
       "    0.0027350330391220476,\n",
       "    0.002101327141452541,\n",
       "    -0.0070090659823953795,\n",
       "    0.023495866898850592,\n",
       "    0.018147666712537338,\n",
       "    0.013293897318398903,\n",
       "    -0.012792503027063067,\n",
       "    0.0037082241892222208,\n",
       "    -0.019345440722298355,\n",
       "    0.0008343504483957659,\n",
       "    0.027660220073724862,\n",
       "    ...],\n",
       "   'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023',\n",
       "   'metadata': {'source': 'sample_doc/doc.pdf', 'page': 0}}},\n",
       " {'_index': 'semantic-index',\n",
       "  '_id': 'doc4',\n",
       "  '_score': 1.7932174,\n",
       "  '_source': {'vector_field': [-0.04368756205587395,\n",
       "    0.012342954608999633,\n",
       "    0.012444458521985753,\n",
       "    -0.027934054922651132,\n",
       "    -0.005163198645001368,\n",
       "    0.014711394456944027,\n",
       "    0.009392555516330644,\n",
       "    -0.02086933638521887,\n",
       "    -0.026702466536996164,\n",
       "    -0.02407688201479236,\n",
       "    0.06160649841710129,\n",
       "    0.04225296398160165,\n",
       "    0.005552299151200771,\n",
       "    0.019894893605145857,\n",
       "    -0.017147503269771654,\n",
       "    -0.0047131952322167665,\n",
       "    0.009764739014139875,\n",
       "    0.008039161317505275,\n",
       "    0.01901518681711584,\n",
       "    -0.04387703776525013,\n",
       "    -0.007748181229481705,\n",
       "    0.004374846682510424,\n",
       "    -0.00481469961086416,\n",
       "    -0.01561816827213381,\n",
       "    0.001451515199077791,\n",
       "    0.020409182953664673,\n",
       "    0.025700953935795745,\n",
       "    -0.02988294234544225,\n",
       "    -0.00956849631514312,\n",
       "    -0.017201639186736277,\n",
       "    0.026296447159761496,\n",
       "    0.02323777716448581,\n",
       "    0.00641508799951807,\n",
       "    -0.005071844285123569,\n",
       "    -0.005998919534836357,\n",
       "    0.007443667627878251,\n",
       "    0.002385357089165202,\n",
       "    -0.02294002962118039,\n",
       "    0.010874521586624443,\n",
       "    0.0010328088804577244,\n",
       "    0.019123458651045082,\n",
       "    0.0077346472502405495,\n",
       "    0.011314374980639452,\n",
       "    0.021315957700177003,\n",
       "    0.0022381755305789087,\n",
       "    0.0068583248883433675,\n",
       "    -0.012004606059293292,\n",
       "    -0.04698984554013553,\n",
       "    0.012464759490847487,\n",
       "    0.01805427801628398,\n",
       "    0.00446958407153724,\n",
       "    0.05161845153944998,\n",
       "    -0.026540058786102296,\n",
       "    -0.0027896835548415406,\n",
       "    -0.019367070277385882,\n",
       "    -0.0022940029621180387,\n",
       "    0.005630119531837415,\n",
       "    0.004388380196090307,\n",
       "    0.004777481167950982,\n",
       "    -0.035161177485687076,\n",
       "    0.012938447832965385,\n",
       "    -0.007592540933869689,\n",
       "    -0.01874450909493782,\n",
       "    0.0032921311371856207,\n",
       "    -0.016782087692905547,\n",
       "    -0.01925879844345664,\n",
       "    -0.004885752536218953,\n",
       "    0.01097602643093311,\n",
       "    0.010326397290002736,\n",
       "    -0.010096320574225638,\n",
       "    0.04328154267863928,\n",
       "    0.017255775103700896,\n",
       "    -0.011003094389415422,\n",
       "    -0.020964074239906958,\n",
       "    0.022926495641939233,\n",
       "    -0.01169332546806926,\n",
       "    -0.022588148023555438,\n",
       "    -0.0185685673648028,\n",
       "    -0.004777481167950982,\n",
       "    -0.01091512352434791,\n",
       "    0.00704441617158671,\n",
       "    -0.020273843161253118,\n",
       "    -0.010312863310761582,\n",
       "    0.03827398526057248,\n",
       "    0.001059030848822145,\n",
       "    -0.00226524348906122,\n",
       "    -0.011124899271263275,\n",
       "    0.019028720796356993,\n",
       "    -0.0002899223966494007,\n",
       "    0.010556474005779835,\n",
       "    0.01188956723574347,\n",
       "    -0.009318118630504288,\n",
       "    0.02874609181447537,\n",
       "    -0.00633388458973241,\n",
       "    0.008106831213711052,\n",
       "    0.0037996539617451337,\n",
       "    -0.0035560430338962435,\n",
       "    0.043849969806767815,\n",
       "    0.0006711989203459646,\n",
       "    0.00687185840192325,\n",
       "    -0.03922136008216318,\n",
       "    -0.007619608892352,\n",
       "    -0.03556719686292174,\n",
       "    -0.018040744037042825,\n",
       "    -0.01509034401305129,\n",
       "    0.006854941393533078,\n",
       "    -0.009311351640883711,\n",
       "    0.01057000798502099,\n",
       "    -0.007321862280369125,\n",
       "    -0.005488013215466555,\n",
       "    -0.015185081867739378,\n",
       "    0.027392697615650004,\n",
       "    0.028935565661206458,\n",
       "    -0.035919080323191785,\n",
       "    -0.014589588643773627,\n",
       "    -0.02299416553814501,\n",
       "    0.005383124876347599,\n",
       "    -0.015076810033810135,\n",
       "    0.013270029393051148,\n",
       "    0.0012772656820091866,\n",
       "    0.018933982941668905,\n",
       "    0.025619750060348813,\n",
       "    0.007037649181966132,\n",
       "    -0.021343025658659313,\n",
       "    0.02057159070455854,\n",
       "    -0.0056267360370271255,\n",
       "    0.015577566334410342,\n",
       "    -0.017445250813077073,\n",
       "    -0.003948527267736571,\n",
       "    -0.021004676177630424,\n",
       "    0.02276408975369046,\n",
       "    0.010860987607383289,\n",
       "    0.0019218196971394444,\n",
       "    0.006472607411292981,\n",
       "    -0.014548986706050161,\n",
       "    0.0047131952322167665,\n",
       "    -0.000838258452735046,\n",
       "    -0.006800805476568457,\n",
       "    -0.018135481891730913,\n",
       "    -0.007254192384163347,\n",
       "    0.026039303416824634,\n",
       "    0.010827152659280399,\n",
       "    -0.004486501545588684,\n",
       "    -0.010725648746294279,\n",
       "    -0.0003723948498195556,\n",
       "    0.035973216240156404,\n",
       "    0.011713626436930994,\n",
       "    0.03483636570918953,\n",
       "    0.0011131666493714488,\n",
       "    -0.0014498234516726464,\n",
       "    0.008627887551850448,\n",
       "    -0.028800227731439993,\n",
       "    0.006039521472559824,\n",
       "    -0.007436900638257674,\n",
       "    0.03656870760147707,\n",
       "    0.018514431447838178,\n",
       "    -0.010529406047297523,\n",
       "    0.0012958749034657753,\n",
       "    -0.02456410340482887,\n",
       "    -0.00366769812980514,\n",
       "    -0.00203178281281256,\n",
       "    0.011578286644519438,\n",
       "    -0.017350512958388985,\n",
       "    0.019867825646663544,\n",
       "    0.007098552088551332,\n",
       "    0.03134460651555177,\n",
       "    0.029476922968207586,\n",
       "    -0.015022675048168059,\n",
       "    0.0019658048968425636,\n",
       "    -0.016579078004288213,\n",
       "    -0.014914403214238814,\n",
       "    0.014765529442586104,\n",
       "    -0.04008753475359714,\n",
       "    -0.003062054188577886,\n",
       "    -0.02434755973697038,\n",
       "    0.005657187490319726,\n",
       "    0.05603051759619613,\n",
       "    -0.020436250912146986,\n",
       "    -0.04303793477758867,\n",
       "    -0.025037790815624218,\n",
       "    0.019340002318903572,\n",
       "    -0.010076019605363905,\n",
       "    0.016700883817458614,\n",
       "    0.024929518981694973,\n",
       "    -0.006090273429052884,\n",
       "    -0.031046860834891443,\n",
       "    0.0316694220173395,\n",
       "    -0.020909938322942336,\n",
       "    0.014603122623014783,\n",
       "    -0.021329491679418156,\n",
       "    0.01607832263501055,\n",
       "    0.03101979287640913,\n",
       "    -0.011957237131949247,\n",
       "    -0.015374557577115555,\n",
       "    -0.6106514148262917,\n",
       "    -0.011212870136330785,\n",
       "    -0.008276005022902952,\n",
       "    -0.037082998812640974,\n",
       "    -0.007227124425681036,\n",
       "    -0.017864802306907804,\n",
       "    0.014224172135584974,\n",
       "    -0.0016113848632026675,\n",
       "    -0.03540478911202787,\n",
       "    0.021099414032318513,\n",
       "    -0.002982542293366734,\n",
       "    -0.0011833739338543065,\n",
       "    -0.024401695653935,\n",
       "    0.009216613786195623,\n",
       "    -0.014345977948755374,\n",
       "    -0.022980631558903855,\n",
       "    -0.00710531907817191,\n",
       "    -0.043092066969263106,\n",
       "    0.0025342303951566398,\n",
       "    0.026891940383727247,\n",
       "    -0.020585124683799697,\n",
       "    0.009020372018521412,\n",
       "    -0.0046116903879081,\n",
       "    0.01352040707768998,\n",
       "    -0.018758043074178977,\n",
       "    0.012153478899623456,\n",
       "    0.017675326597531626,\n",
       "    -0.010637677881226767,\n",
       "    0.005454178267363666,\n",
       "    0.012342954608999633,\n",
       "    -0.04444546116808847,\n",
       "    0.001164764828813036,\n",
       "    0.019096390692562772,\n",
       "    0.015455760521239943,\n",
       "    0.04236123581553089,\n",
       "    -0.012742206065291176,\n",
       "    -0.022466342210385036,\n",
       "    0.011788062391434803,\n",
       "    -0.00027744578864029,\n",
       "    0.052213946626060825,\n",
       "    -0.015415158583516476,\n",
       "    -0.030830317167032953,\n",
       "    0.01694449358115432,\n",
       "    -0.007233891415301614,\n",
       "    0.0049974078649584865,\n",
       "    0.005951550607492312,\n",
       "    0.009798573030920217,\n",
       "    0.007098552088551332,\n",
       "    -0.008221869105938329,\n",
       "    -0.02150543340955318,\n",
       "    -0.0234543208323443,\n",
       "    -0.004378230177320713,\n",
       "    -0.007010581689145094,\n",
       "    -0.003439312695771914,\n",
       "    0.0008555988636377765,\n",
       "    0.003459613664633647,\n",
       "    0.029693466636066073,\n",
       "    -0.0032210779790001905,\n",
       "    0.026309981139002653,\n",
       "    0.004821466600484737,\n",
       "    0.006898926360405561,\n",
       "    0.020761066413934718,\n",
       "    -0.010867754597003865,\n",
       "    -0.00598876905040549,\n",
       "    -0.02383327038845156,\n",
       "    -0.008113597272009086,\n",
       "    0.011970771111190402,\n",
       "    0.007010581689145094,\n",
       "    0.012336187619379055,\n",
       "    -0.026729534495478473,\n",
       "    -0.010116620611764826,\n",
       "    0.014251240094067285,\n",
       "    0.013337699289256925,\n",
       "    0.001232434492188177,\n",
       "    0.02482124901041082,\n",
       "    0.027230289864756136,\n",
       "    0.021370093617141626,\n",
       "    -0.006330501094922121,\n",
       "    0.007396298700534207,\n",
       "    0.03383485124534402,\n",
       "    0.02084227028938165,\n",
       "    -0.029964146220889182,\n",
       "    -0.010928657503589066,\n",
       "    0.025132528670312307,\n",
       "    -0.010705347777432546,\n",
       "    0.011314374980639452,\n",
       "    -0.015712905195499352,\n",
       "    0.029910010303924563,\n",
       "    -0.007172988974377687,\n",
       "    -0.0009845941958264261,\n",
       "    0.007842918618508521,\n",
       "    0.004513569504070995,\n",
       "    -0.02622877726355572,\n",
       "    -0.06312230036682052,\n",
       "    0.005007558349389353,\n",
       "    0.04173867277043774,\n",
       "    0.002513929659125543,\n",
       "    0.022452808231143883,\n",
       "    0.017377580916871298,\n",
       "    -0.010394067186208514,\n",
       "    -0.022141528571242398,\n",
       "    -0.00023895865158857224,\n",
       "    0.004452666597485795,\n",
       "    -0.00830983997100584,\n",
       "    0.016538476066564747,\n",
       "    0.024415229633176157,\n",
       "    -0.016782087692905547,\n",
       "    0.007254192384163347,\n",
       "    0.016903891643430854,\n",
       "    -0.022033256737313153,\n",
       "    -0.03134460651555177,\n",
       "    -0.011842198308399426,\n",
       "    -0.005163198645001368,\n",
       "    0.013533941056931136,\n",
       "    0.004554170976133189,\n",
       "    -0.03968151537636247,\n",
       "    0.037164202688087906,\n",
       "    -0.016010652738804775,\n",
       "    -0.0009930529328521483,\n",
       "    -0.0019996398449454523,\n",
       "    0.014197104177102664,\n",
       "    -0.01614599253121633,\n",
       "    0.004391763690900595,\n",
       "    -0.02233100241797348,\n",
       "    0.009067740945865457,\n",
       "    0.0008754768374405645,\n",
       "    0.0069293778136981605,\n",
       "    -0.0201249712522455,\n",
       "    -0.013899357565119789,\n",
       "    0.03637923561739107,\n",
       "    0.008729392396159115,\n",
       "    -0.010339931269243893,\n",
       "    0.02787991900568651,\n",
       "    -0.018703907157214355,\n",
       "    -0.011226404115571941,\n",
       "    0.0026052837861727063,\n",
       "    -0.003914692319633682,\n",
       "    0.0022449425201994866,\n",
       "    -0.007301561311507392,\n",
       "    -0.00509891224360588,\n",
       "    -0.007321862280369125,\n",
       "    0.005132747191708769,\n",
       "    0.020909938322942336,\n",
       "    -0.010082786594984483,\n",
       "    -0.02790698696416882,\n",
       "    -0.010664745839709078,\n",
       "    -0.027934054922651132,\n",
       "    -0.005921099154199713,\n",
       "    0.001707814232465264,\n",
       "    0.004557554470943478,\n",
       "    0.00044915768460117766,\n",
       "    -0.016809155651387856,\n",
       "    -0.0029926927777976006,\n",
       "    -0.004628607861959544,\n",
       "    0.0011782988080541916,\n",
       "    -0.02183024704869582,\n",
       "    0.00881736232990408,\n",
       "    -0.019231730484974327,\n",
       "    -0.00738953171091363,\n",
       "    -0.045446975631933985,\n",
       "    0.01176776142257307,\n",
       "    0.03941083579153936,\n",
       "    -0.029368652996923435,\n",
       "    -0.0024428762681094765,\n",
       "    -0.027054348134621115,\n",
       "    0.005691022438422615,\n",
       "    -0.028313004478758393,\n",
       "    0.020991142198389268,\n",
       "    -0.01006248562612275,\n",
       "    -0.024591171363311178,\n",
       "    -0.015482828479722254,\n",
       "    -0.04284845906821249,\n",
       "    0.012234682775070388,\n",
       "    0.0009135410376409961,\n",
       "    -0.004598156408666944,\n",
       "    0.02204679071655431,\n",
       "    0.003539125559844799,\n",
       "    0.0067162181063112345,\n",
       "    0.00019740522236503017,\n",
       "    -0.016795621672146703,\n",
       "    0.03123633654426762,\n",
       "    -0.013263262403430572,\n",
       "    -0.022655817919761213,\n",
       "    -0.0037286008035597035,\n",
       "    0.033374699676434916,\n",
       "    0.006117341387535194,\n",
       "    0.006059821975760283,\n",
       "    -0.005000791359768775,\n",
       "    -0.01254596336629442,\n",
       "    0.01590238090487553,\n",
       "    0.01176776142257307,\n",
       "    -0.002958857829694712,\n",
       "    -0.029476922968207586,\n",
       "    0.0038030374565554224,\n",
       "    -0.0013474729664920443,\n",
       "    0.008485780769818315,\n",
       "    0.02721675588551498,\n",
       "    -0.010089553584605062,\n",
       "    -0.014657258539979406,\n",
       "    0.035702536655333295,\n",
       "    0.019394138235868195,\n",
       "    0.002715246901845822,\n",
       "    0.0033817935168276394,\n",
       "    -0.019448274152832817,\n",
       "    -0.001928586570344704,\n",
       "    -0.01877157705342013,\n",
       "    0.0031161898727118716,\n",
       "    -0.028800227731439993,\n",
       "    0.01563170132005242,\n",
       "    0.004831617084915604,\n",
       "    0.02066632855924663,\n",
       "    -0.005511697679138577,\n",
       "    -0.025470878151341195,\n",
       "    0.007721113270999394,\n",
       "    -0.005328989425044251,\n",
       "    0.057600455462879986,\n",
       "    -0.008167733188973706,\n",
       "    0.011300841001398295,\n",
       "    -0.018906914983186595,\n",
       "    0.01002865067801986,\n",
       "    0.008404577825693928,\n",
       "    -0.022317468438732325,\n",
       "    0.002212799319501742,\n",
       "    0.024388161674693844,\n",
       "    0.00018968663938097343,\n",
       "    -0.00606997246019115,\n",
       "    0.007856452597749676,\n",
       "    0.020612192642282007,\n",
       "    0.007727880260619971,\n",
       "    0.004970339906476175,\n",
       "    -0.01242415755312402,\n",
       "    0.01345950417110478,\n",
       "    0.02681073837092541,\n",
       "    0.030207755984584892,\n",
       "    0.014400113865719997,\n",
       "    0.0028793459344835597,\n",
       "    0.030478435569408,\n",
       "    -0.005822978270362608,\n",
       "    0.024117483952515825,\n",
       "    -0.015821177029428597,\n",
       "    0.0026458854910655365,\n",
       "    0.020476852849870452,\n",
       "    -0.017269309082942053,\n",
       "    -0.008810595340283502,\n",
       "    0.00956849631514312,\n",
       "    0.005900798185337979,\n",
       "    0.02754156952465762,\n",
       "    0.004219206386898409,\n",
       "    -0.01745878479231823,\n",
       "    -0.027825783088721887,\n",
       "    -0.00821510211631775,\n",
       "    0.01421063815634382,\n",
       "    0.006997047709903939,\n",
       "    0.001145309733653875,\n",
       "    0.014873801276515348,\n",
       "    0.0035797274975682657,\n",
       "    0.0008048465578987615,\n",
       "    -0.007782016177584594,\n",
       "    0.03277920458982407,\n",
       "    0.023589658762110763,\n",
       "    0.00553199864800031,\n",
       "    -0.009845941958264262,\n",
       "    -0.0014709701778216338,\n",
       "    -0.008343674919108729,\n",
       "    0.004212439397277831,\n",
       "    -0.0004813007688836038,\n",
       "    -0.005010941844199642,\n",
       "    -0.026174641346591098,\n",
       "    -0.026431786952173054,\n",
       "    -0.018703907157214355,\n",
       "    -0.006655315665387308,\n",
       "    -0.011774528412193647,\n",
       "    0.003099272631491064,\n",
       "    -0.017404648875353607,\n",
       "    -0.01661967994201168,\n",
       "    0.022303934459491172,\n",
       "    -0.011551218686037127,\n",
       "    0.021667839297801954,\n",
       "    9.695800394691727e-05,\n",
       "    0.04279432315124787,\n",
       "    -0.014603122623014783,\n",
       "    -0.035431857070510185,\n",
       "    0.0030400614723110086,\n",
       "    -0.007788783167205171,\n",
       "    0.017445250813077073,\n",
       "    -0.019394138235868195,\n",
       "    -0.004740262725037804,\n",
       "    0.0075113375240840295,\n",
       "    -0.007288027332266236,\n",
       "    0.004148152995882342,\n",
       "    -0.020273843161253118,\n",
       "    0.000743943825936539,\n",
       "    0.004486501545588684,\n",
       "    -0.01894751692091006,\n",
       "    -0.02955812684365452,\n",
       "    0.005518464668759155,\n",
       "    0.029476922968207586,\n",
       "    -0.02311597135131541,\n",
       "    0.033510039468846474,\n",
       "    -0.027135552010068047,\n",
       "    -0.017296377041424366,\n",
       "    -0.006046287996519128,\n",
       "    -0.02612050729227157,\n",
       "    -0.0005654649985627327,\n",
       "    0.03491756958463646,\n",
       "    -0.0004893365399542103,\n",
       "    -0.016687349838217458,\n",
       "    -0.007010581689145094,\n",
       "    0.0006043750724657366,\n",
       "    -0.010015116698778706,\n",
       "    0.020219707244288496,\n",
       "    -0.014995607089685748,\n",
       "    -0.018622703281767423,\n",
       "    0.015415158583516476,\n",
       "    0.008621120562229872,\n",
       "    0.004428982133813773,\n",
       "    -0.01958361208259928,\n",
       "    0.0008890107584740609,\n",
       "    0.01970541789576968,\n",
       "    0.008039161317505275,\n",
       "    0.006354185558594143,\n",
       "    -0.0166738158589763,\n",
       "    0.009832407979023105,\n",
       "    0.013635445901239801,\n",
       "    0.028854361785759522,\n",
       "    0.04328154267863928,\n",
       "    -0.045501111548898604,\n",
       "    0.024807715031169665,\n",
       "    -0.034267938581060996,\n",
       "    0.0028827294292938484,\n",
       "    -0.030776183112713424,\n",
       "    -0.024699443197240423,\n",
       "    0.011111365292022118,\n",
       "    0.003600028466429999,\n",
       "    -0.02407688201479236,\n",
       "    -0.044472529126570785,\n",
       "    -0.0034088614753099503,\n",
       "    -0.01028579535227927,\n",
       "    0.02985587438695994,\n",
       "    0.0069293778136981605,\n",
       "    -0.018203151787936692,\n",
       "    -8.453426304421735e-05,\n",
       "    0.010989560410174265,\n",
       "    0.013858755627396321,\n",
       "    0.018703907157214355,\n",
       "    -0.013939959502843255,\n",
       "    0.011490315779451928,\n",
       "    0.0196377479995639,\n",
       "    0.01597005080108131,\n",
       "    0.0035357420650345105,\n",
       "    0.014007629399049032,\n",
       "    0.03878827647173639,\n",
       "    0.022412206293420413,\n",
       "    -0.03513410952720476,\n",
       "    0.03683938718630018,\n",
       "    0.0041109345529691645,\n",
       "    0.002129903929480301,\n",
       "    0.012153478899623456,\n",
       "    -0.006915843834457005,\n",
       "    0.021464831471829714,\n",
       "    -0.007545172006525645,\n",
       "    0.000955834606354289,\n",
       "    0.031994237519127236,\n",
       "    0.005271470013269339,\n",
       "    -0.0013906124089079093,\n",
       "    0.036406303575873385,\n",
       "    0.0008501006845710571,\n",
       "    0.005680871953991748,\n",
       "    -0.004655675820441856,\n",
       "    -0.008106831213711052,\n",
       "    -0.010488804109574057,\n",
       "    0.011557985675657705,\n",
       "    -0.011449713841728461,\n",
       "    0.005850046228844919,\n",
       "    0.006601179748422685,\n",
       "    -0.014373045907237685,\n",
       "    -0.043443950429533154,\n",
       "    -0.010874521586624443,\n",
       "    0.0016401444526748046,\n",
       "    -0.024212219944558824,\n",
       "    0.001090328175817317,\n",
       "    -0.009094808904347768,\n",
       "    0.0004087673317186724,\n",
       "    -0.015848244987910907,\n",
       "    -0.00404326512242466,\n",
       "    -0.017079833373565875,\n",
       "    0.020625726621523163,\n",
       "    -0.004875602517449359,\n",
       "    0.029937078262406873,\n",
       "    -0.009176011848472155,\n",
       "    -0.02032797907821774,\n",
       "    0.00366769812980514,\n",
       "    -0.012133177930761723,\n",
       "    -0.007281260342645659,\n",
       "    -0.012816642019794985,\n",
       "    0.003050211956741875,\n",
       "    -0.040818365907329346,\n",
       "    -0.01910992467180393,\n",
       "    0.015063276985891525,\n",
       "    0.024631773301034644,\n",
       "    0.018270819821497378,\n",
       "    0.005115829717657324,\n",
       "    -0.016010652738804775,\n",
       "    -0.0011131666493714488,\n",
       "    -0.011185802177848474,\n",
       "    -0.017445250813077073,\n",
       "    0.002617125785178081,\n",
       "    -0.02622877726355572,\n",
       "    -0.022168596529724707,\n",
       "    0.01743171683383592,\n",
       "    0.018040744037042825,\n",
       "    -0.01676855371366439,\n",
       "    0.0017847885065686996,\n",
       "    -0.015685837237017042,\n",
       "    -0.005511697679138577,\n",
       "    0.017594122722084694,\n",
       "    -0.0017712545273275441,\n",
       "    -0.012126410941141145,\n",
       "    0.028529548146616884,\n",
       "    0.02787991900568651,\n",
       "    -0.0008623657950506951,\n",
       "    0.01266776917946482,\n",
       "    0.03372658313670496,\n",
       "    -0.024171619869480448,\n",
       "    0.002464868984376354,\n",
       "    -0.032210781186985726,\n",
       "    0.024929518981694973,\n",
       "    -0.008864731257248125,\n",
       "    0.02294002962118039,\n",
       "    -0.015685837237017042,\n",
       "    -0.00412446853221032,\n",
       "    0.029476922968207586,\n",
       "    0.00223479203576862,\n",
       "    -0.001228205123675316,\n",
       "    0.009649700190590053,\n",
       "    -0.040953705699740904,\n",
       "    0.00584666273403463,\n",
       "    0.010982793420553687,\n",
       "    0.01364221289086038,\n",
       "    -0.003046828461931586,\n",
       "    0.013364767247739237,\n",
       "    -0.0024208837846732352,\n",
       "    -0.02254754608583197,\n",
       "    -0.005853429723655208,\n",
       "    -0.005701172922853481,\n",
       "    -0.015564032355169188,\n",
       "    0.03884241238870101,\n",
       "    0.003347958801555387,\n",
       "    -0.007917355504334875,\n",
       "    0.02545734417210004,\n",
       "    0.011118132281642696,\n",
       "    -0.024658841259516957,\n",
       "    -0.022344536397214638,\n",
       "    0.031155132668820685,\n",
       "    -0.0024919369428586654,\n",
       "    0.04644848637048931,\n",
       "    0.012451225511606332,\n",
       "    -0.006919227329267294,\n",
       "    -0.004009430174321771,\n",
       "    0.00027363938608254457,\n",
       "    -0.030830317167032953,\n",
       "    -0.0007498649418545446,\n",
       "    -0.02287235972497461,\n",
       "    -0.0009152327850461406,\n",
       "    0.011273773042915986,\n",
       "    0.003471455896469658,\n",
       "    -0.0008555988636377765,\n",
       "    -0.00633388458973241,\n",
       "    0.024293423820005756,\n",
       "    -0.028096462673544997,\n",
       "    -0.021600169401596175,\n",
       "    0.0023464471316775163,\n",
       "    -0.003303973369021632,\n",
       "    0.024712977176481576,\n",
       "    -0.005119213212467613,\n",
       "    5.841798887485266e-05,\n",
       "    -0.028989701578171077,\n",
       "    -0.021234753824730068,\n",
       "    -0.014129434280896885,\n",
       "    -0.022750555774449302,\n",
       "    -0.002569757090664673,\n",
       "    -0.012295585681655589,\n",
       "    0.011876033256502314,\n",
       "    0.017350512958388985,\n",
       "    0.047287589358150764,\n",
       "    0.011869266266881735,\n",
       "    0.02728442578172076,\n",
       "    0.01925879844345664,\n",
       "    -0.01598358478032246,\n",
       "    -0.0021265204346700125,\n",
       "    -0.006178244294120394,\n",
       "    0.003305665116426776,\n",
       "    -0.040953705699740904,\n",
       "    0.005501547194707711,\n",
       "    0.006783888002517012,\n",
       "    0.007409832679775363,\n",
       "    0.002495320437668954,\n",
       "    0.0034207037071459613,\n",
       "    0.036649911476924,\n",
       "    0.026161109229995035,\n",
       "    -0.004398530680521174,\n",
       "    -0.008499314749059472,\n",
       "    -0.016240728523259324,\n",
       "    -0.02164077133931964,\n",
       "    0.011720393426551571,\n",
       "    0.02005730135603972,\n",
       "    -0.0278528510472042,\n",
       "    -0.012133177930761723,\n",
       "    -0.018297887779979687,\n",
       "    0.010488804109574057,\n",
       "    0.017594122722084694,\n",
       "    0.009203079806954466,\n",
       "    -0.002069001255725738,\n",
       "    0.04057475428098855,\n",
       "    0.018365557676185466,\n",
       "    0.0002043836658593362,\n",
       "    -0.010272261373038114,\n",
       "    0.03039723169396107,\n",
       "    0.0368935231032648,\n",
       "    -0.008675256479194492,\n",
       "    -0.0105970759435033,\n",
       "    -0.030938589000962198,\n",
       "    -0.021924984903383907,\n",
       "    0.050968824261164704,\n",
       "    0.019177594568009704,\n",
       "    0.002735547870707555,\n",
       "    -0.012776040082071519,\n",
       "    0.00575530837415683,\n",
       "    -0.01082038566965982,\n",
       "    0.00799179239016123,\n",
       "    -0.01743171683383592,\n",
       "    -0.018297887779979687,\n",
       "    0.0018981352334674223,\n",
       "    -0.03746195023139333,\n",
       "    -0.021776111131731196,\n",
       "    -0.0166738158589763,\n",
       "    -0.02502425683638306,\n",
       "    -0.020788134372417028,\n",
       "    0.013601610953136913,\n",
       "    0.017675326597531626,\n",
       "    -0.011145200240125008,\n",
       "    0.001141926238843586,\n",
       "    -0.016308398419465103,\n",
       "    -0.030613775361819556,\n",
       "    0.02842127631268764,\n",
       "    0.005785759827449431,\n",
       "    0.057925267239377534,\n",
       "    -0.011124899271263275,\n",
       "    -0.003955294257357149,\n",
       "    0.018270819821497378,\n",
       "    0.013872289606637478,\n",
       "    -0.0017298068323168235,\n",
       "    0.005711323407284348,\n",
       "    -0.020909938322942336,\n",
       "    -0.010725648746294279,\n",
       "    0.022912961662698076,\n",
       "    0.01841969359315009,\n",
       "    -0.006256064209095765,\n",
       "    -0.016091856614251707,\n",
       "    -0.006831256929861056,\n",
       "    -0.006577495284750663,\n",
       "    -0.002481786458427799,\n",
       "    -0.04893873110028155,\n",
       "    0.0232107092060035,\n",
       "    0.024794181051928512,\n",
       "    -0.0026661864599272695,\n",
       "    0.0017179646004808124,\n",
       "    -0.009778272062058484,\n",
       "    -0.006076739449811728,\n",
       "    -0.0075790069546285335,\n",
       "    -0.006107190903104328,\n",
       "    0.011585053634140016,\n",
       "    0.0041312355218308975,\n",
       "    -0.009798573030920217,\n",
       "    -0.02901676953665339,\n",
       "    0.021478365451070867,\n",
       "    0.015523430417445722,\n",
       "    0.016159526510457486,\n",
       "    0.02050392080835276,\n",
       "    -0.006651932170577019,\n",
       "    -0.0010683354595504395,\n",
       "    0.003813187940986289,\n",
       "    -0.020991142198389268,\n",
       "    0.02162723736007849,\n",
       "    -0.014616656602255938,\n",
       "    0.02257461404431428,\n",
       "    -0.03183182976823337,\n",
       "    0.048207899946549344,\n",
       "    0.0229670975796627,\n",
       "    -0.02640471899369074,\n",
       "    0.011673024499207527,\n",
       "    0.013351233268498082,\n",
       "    0.010042184657261017,\n",
       "    0.02754156952465762,\n",
       "    -0.04685450574772398,\n",
       "    -0.003786119982503978,\n",
       "    0.019989631459833946,\n",
       "    -0.02093700628142465,\n",
       "    0.01166625750958695,\n",
       "    -0.01197753810081098,\n",
       "    0.0062086957474129945,\n",
       "    -0.011984305090431559,\n",
       "    -0.026756602453960786,\n",
       "    -0.015794109070946284,\n",
       "    0.034484482248919486,\n",
       "    0.025998701479101168,\n",
       "    0.0006796575991640277,\n",
       "    0.007572239965007956,\n",
       "    0.011057230306380042,\n",
       "    -0.005508314184328288,\n",
       "    -0.016755019734423237,\n",
       "    0.00472334525098636,\n",
       "    0.037516086148357955,\n",
       "    -0.035431857070510185,\n",
       "    -0.010576774974641568,\n",
       "    0.007626375881972578,\n",
       "    -0.008898566205351012,\n",
       "    0.03754315410684026,\n",
       "    -0.013290330361912881,\n",
       "    -0.019245264464215484,\n",
       "    0.017688860576772783,\n",
       "    0.03781382996637319,\n",
       "    -0.001522568357263221,\n",
       "    -0.01674148575518208,\n",
       "    0.017702394556013936,\n",
       "    -0.019055788754839306,\n",
       "    0.0038030374565554224,\n",
       "    -0.004608306893097811,\n",
       "    -0.0010125080280113095,\n",
       "    -0.02015203921072781,\n",
       "    0.00023472929762762592,\n",
       "    -0.01572643917474051,\n",
       "    -0.009047439977003724,\n",
       "    -0.0024327260165092463,\n",
       "    -0.004675976789303589,\n",
       "    0.010908356534727333,\n",
       "    0.006658699160197596,\n",
       "    0.01116550120898674,\n",
       "    -0.0017255774638039625,\n",
       "    -0.0031381825889787497,\n",
       "    0.002933481851448182,\n",
       "    -0.012924913853724228,\n",
       "    0.022723487815966992,\n",
       "    0.005657187490319726,\n",
       "    0.015699371216258196,\n",
       "    -0.030965656959444508,\n",
       "    0.012938447832965385,\n",
       "    -0.01877157705342013,\n",
       "    0.021302423720935847,\n",
       "    -0.015469294500481099,\n",
       "    -0.014454248851362072,\n",
       "    -0.0061883947785512615,\n",
       "    -0.042144692147672407,\n",
       "    -0.010042184657261017,\n",
       "    -0.005041393297492242,\n",
       "    -0.006807572466189034,\n",
       "    -0.0307220471957488,\n",
       "    0.00821510211631775,\n",
       "    0.013452737181484202,\n",
       "    -0.012349721598620212,\n",
       "    -0.011781295401814225,\n",
       "    0.008627887551850448,\n",
       "    0.009866242927125995,\n",
       "    -0.015279819722427467,\n",
       "    -0.006503058864585581,\n",
       "    0.014494850789085538,\n",
       "    0.004100784068538298,\n",
       "    -0.0013906124089079093,\n",
       "    -0.024956586940177286,\n",
       "    -0.03443034633195486,\n",
       "    0.010360232238105626,\n",
       "    0.0036034117284096515,\n",
       "    -0.013256495413809993,\n",
       "    -0.024063348035551206,\n",
       "    -0.01812194791248976,\n",
       "    0.0395191076254686,\n",
       "    0.03064084332030187,\n",
       "    -0.000314664144241979,\n",
       "    0.003525591580603644,\n",
       "    -0.010394067186208514,\n",
       "    -0.05156431562248536,\n",
       "    -0.017323444999906675,\n",
       "    -0.02224979854252655,\n",
       "    0.006753436549224412,\n",
       "    0.010766249752695198,\n",
       "    0.02671600051623732,\n",
       "    -0.014345977948755374,\n",
       "    0.0025443808795875063,\n",
       "    0.013723416766307313,\n",
       "    -0.005071844285123569,\n",
       "    -0.031994237519127236,\n",
       "    0.005034626307871664,\n",
       "    -0.002388740583975491,\n",
       "    -0.007037649181966132,\n",
       "    -0.007172988974377687,\n",
       "    -0.0030705129256036086,\n",
       "    -0.036649911476924,\n",
       "    -0.0025562231114235174,\n",
       "    -0.0092233807758162,\n",
       "    -0.002344755384272372,\n",
       "    0.006185011283740972,\n",
       "    0.03348297151036416,\n",
       "    -0.00478424815757156,\n",
       "    -0.010590308953882722,\n",
       "    -0.013148223579880748,\n",
       "    0.049859037963389946,\n",
       "    0.016565544025047056,\n",
       "    0.014711394456944027,\n",
       "    0.013791085731190544,\n",
       "    -0.03245438908803634,\n",
       "    -0.011131666260883853,\n",
       "    -0.0025528396166132287,\n",
       "    -0.011023395358277155,\n",
       "    -0.007112086067792487,\n",
       "    0.014467782830603229,\n",
       "    0.022885893704215766,\n",
       "    0.0008162658528834865,\n",
       "    0.0037962704669348445,\n",
       "    -0.023887406305416182,\n",
       "    -0.007118853057413065,\n",
       "    0.012315886650517322,\n",
       "    -0.01338506821660097,\n",
       "    0.03624389582497952,\n",
       "    0.02294002962118039,\n",
       "    -0.003459613664633647,\n",
       "    0.030207755984584892,\n",
       "    0.026621262661549232,\n",
       "    0.011774528412193647,\n",
       "    0.007220357436060459,\n",
       "    -0.008837663298765813,\n",
       "    -0.01850089746859702,\n",
       "    -0.008066229275987586,\n",
       "    0.02395507620162196,\n",
       "    -0.005193650098293969,\n",
       "    0.0006166401829453747,\n",
       "    0.007890287545852566,\n",
       "    0.006337268084542698,\n",
       "    -0.027067882113862272,\n",
       "    -0.011835431318778847,\n",
       "    0.0153610235978744,\n",
       "    0.003931609793685127,\n",
       "    -0.027649841358586866,\n",
       "    0.009115108941886956,\n",
       "    -0.015036209027409214,\n",
       "    -0.030261891901549515,\n",
       "    0.017594122722084694,\n",
       "    -0.024929518981694973,\n",
       "    0.004066949120435409,\n",
       "    -0.01082038566965982,\n",
       "    0.004104167563348587,\n",
       "    0.008282772012523528,\n",
       "    0.010055718636502172,\n",
       "    0.0056267360370271255,\n",
       "    0.015767041112463975,\n",
       "    0.0010886364284121727,\n",
       "    -0.014846733318033037,\n",
       "    -0.021153549949283135,\n",
       "    0.010576774974641568,\n",
       "    4.393244173606904e-05,\n",
       "    -0.003928226298874838,\n",
       "    0.17789011650628314,\n",
       "    -0.017255775103700896,\n",
       "    -0.007497803544842874,\n",
       "    0.013621911921998646,\n",
       "    0.005396658855588755,\n",
       "    -0.031073928793373753,\n",
       "    -0.0077549482191022825,\n",
       "    -0.007538405016905067,\n",
       "    -0.0010023575435804428,\n",
       "    -0.009148943889989844,\n",
       "    0.00979180604129964,\n",
       "    -0.00239212407878578,\n",
       "    -0.014291842031790753,\n",
       "    -0.0015132637465349265,\n",
       "    0.01877157705342013,\n",
       "    0.0005100603874595706,\n",
       "    -0.04225296398160165,\n",
       "    -0.041440928952422504,\n",
       "    -0.011442946852107883,\n",
       "    0.011957237131949247,\n",
       "    -0.0036981493502671035,\n",
       "    -0.00859405260374756,\n",
       "    -0.0051090627280367465,\n",
       "    -0.03064084332030187,\n",
       "    0.013283563372292305,\n",
       "    -0.00509891224360588,\n",
       "    0.004767330683520116,\n",
       "    -0.005115829717657324,\n",
       "    0.009602331263246009,\n",
       "    0.01970541789576968,\n",
       "    -0.018040744037042825,\n",
       "    0.0016553701793211046,\n",
       "    0.01132114197026003,\n",
       "    0.018311421759220844,\n",
       "    -0.02533553835892964,\n",
       "    -0.0076805117989372,\n",
       "    0.0018203150856614147,\n",
       "    -0.024591171363311178,\n",
       "    0.03559426482140405,\n",
       "    -0.002038549802433138,\n",
       "    0.029449856872370367,\n",
       "    0.011788062391434803,\n",
       "    0.015130945950774757,\n",
       "    -0.013851988637775745,\n",
       "    0.006743286064793546,\n",
       "    0.006120724882345483,\n",
       "    ...],\n",
       "   'text': 'output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5',\n",
       "   'metadata': {'source': 'sample_doc/doc.pdf', 'page': 4}}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensearch_vector_search.similarity_search(\n",
    "    index_name=\"semantic-index\",\n",
    "    query=\"what is Scaled Dot-Product Attention?\",\n",
    "    top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_apps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
